{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning Part 1\n",
    "\n",
    "<center><img width=1000 src=\"https://media3.giphy.com/media/PEXjePkLmKcy4/giphy.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://am2.co/wp-content/uploads/MathCaution.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "<center><img src=\"https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/What-is-Supervised-Learning-Machine-Learning-Tutorial-Edureka.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "\n",
    "<center><img src=\"https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Unsupervised-Learning-Machine-Learning-Tutorial.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "<center><img src='https://cdn-images-1.medium.com/max/1000/1*vz3AN1mBUR2cr_jEG8s7Mg.png' width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General Problem\n",
    "\n",
    "* Maximize our reward\n",
    "* Rewards are sparse and often delayed\n",
    "* There isn't always a clear best action\n",
    "* Very little supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/lazy_opt.png?raw=true\" width=1400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Atari\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1200/1*qFHnCDhep6OmqkbVN6NY_g.gif\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Go\n",
    "<center><img src='https://storage.googleapis.com/deepmind-live-cms-alt/documents/Knowledge%2520Timeline.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://storage.googleapis.com/deepmind-live-cms-alt/documents/AZ-Blog-Fig1-Generality-Performance-Across-Games.gif\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Starcraft\n",
    "\n",
    "<center><img src='https://storage.googleapis.com/deepmind-live-cms-alt/documents/sc2-agent-vis%2520%25281%2529.gif' width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Capture the Flag\n",
    "\n",
    "<center><img src='https://storage.googleapis.com/deepmind-live-cms-alt/documents/science_results_gameplay.gif' width=1200> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why it's popular now?\n",
    "\n",
    "<center><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQYMW7qyl4GMrLvpZ0cWC5GydBmKsrdxpLbIpPMKmaIP1kvx3UP\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Advances in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Advances in reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Advances in computational capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What has RL achieved?\n",
    "\n",
    "* Acquire high degree of proficiency in domains governed by simple, known rules\n",
    "\n",
    "* Learn simple skills with raw sensory inputs, given enough experience\n",
    "* Learn from imitating enough humanprovided expert behavior\n",
    "* Basically:\n",
    "    * Video games\n",
    "    * Simulated environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are the problems?\n",
    "* Humans can learn incredibly quickly\n",
    "    * Deep RL methods are usually slow\n",
    "* Humans can reuse past knowledge\n",
    "    * Transfer learning in deep RL is an open problem\n",
    "* Not clear what the reward function should be\n",
    "* Not clear what the role of prediction should be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inferring Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"600\" src=\"https://www.youtube.com/embed/Z-eU5xZW7cU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"100%\" height=\"600\" src=\"https://www.youtube.com/embed/Z-eU5xZW7cU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Robotics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"618\" src=\"https://www.youtube.com/embed/iaF43Ze1oeI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"100%\" height=\"600\" src=\"https://www.youtube.com/embed/iaF43Ze1oeI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where do rewards come from?\n",
    "\n",
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/basal_ganglia.png?raw=true\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Forms of Supervision\n",
    "\n",
    "* Learning from demonstrations\n",
    "     * Directly copying observed behavior\n",
    "     * Inferring rewards from observed behavior (inverse reinforcement learning)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Learning from observing the world\n",
    "    * Learning to predict\n",
    "    * Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Learning from other tasks\n",
    "    * Transfer learning\n",
    "    * Meta-learning: learning to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Decision Process\n",
    "* Mathematical formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $S$: set of possible states\n",
    "* $A$: set of possible actions\n",
    "* $R$: distribution of reward given (state, action) pair\n",
    "* $P$: transition probability i.e. distribution over next state given (state, action) pair\n",
    "* $\\gamma$: discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Atari Games\n",
    "\n",
    "<center><img src='https://www.retrogamer.net/wp-content/uploads/2014/07/Top-10-Atari-Jaguar-Games-616x410.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Goal:** Maximize the score\n",
    "* **State:** Raw pixels of the game screen\n",
    "* **Actions:** Joystick controls/buttons\n",
    "* **Reward:** Game score at each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Terminology and Notation\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1000/1*vz3AN1mBUR2cr_jEG8s7Mg.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $s_t$ - State: pixels on the screen (what Mario sees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $a_t$ - Action: for Mario right, left, up, down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\pi_{\\theta}(a_t | s_t)$ - Policy: what action to take given the state we observe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $r(s, a)$ - Reward: how good it is to be in state $s_t$ and take action $a_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $p(s_{t+1} | s_t, a_t)$ - Transition dynamics: probability of going to the next state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\tau = s_1, a_1, ..., s_T, a_T$ - Finite Trajectory: sequence of states & actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review Expectations\n",
    "* [Visual example](https://seeing-theory.brown.edu/basic-probability/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goal of Reinforcement Learning\n",
    "\n",
    "$\\pi_\\theta(\\tau) = p_{\\theta}(s_1, a_1, ..., s_T, a_T) = p(s_1) \\prod_{t=1}^T \\pi_{\\theta} (a_t | s_t) p(s_{t+1} |s_t, a_t)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\theta^{*} = argmax_{\\theta} E_{\\tau \\sim \\pi_{\\theta}(\\tau)}[\\sum_t r(s_t, a_t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Find the parameters that give us the highest expected reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sometimes it's very hard to know $p(s_{t+1}|s_t, a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Immitation Learning\n",
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/imitation_learning.png?raw=true\">\n",
    "\n",
    "Bojarski et al.'16, NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/imitation_learning_nvidia.png?raw=true\" width=800>\n",
    "\n",
    "Bojarski et al.'16, NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameterized Policies\n",
    "* $\\Pi = \\{\\pi_{\\theta}, \\theta \\in R^m \\}$\n",
    "* \"A policy, $\\pi_{\\theta}$, with parameters $\\theta$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\\pi_\\theta$ is mario. The policy mario follows, $\\pi$, is determined by a set of parameters $\\theta$, **you** (the player). If you change $\\theta$ mario will take different actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Gradients\n",
    "* Problems with Q-learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Requires a descrete set of actions\n",
    "* Hard to learn exact value of every (state, action) pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Instead we want to learn a policy\n",
    "* Tells us directly what actions to take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluate a Policy\n",
    "* **Objective:** $\\theta^{*} = argmax_{\\theta} E_{\\tau \\sim \\pi_{\\theta}(\\tau)}[\\sum_t r(s_t, a_t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $J(\\theta) = E_{\\tau \\sim \\pi_{\\theta}(\\tau)}[\\sum_t r(s_t, a_t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We want to approximate $J(\\theta)$ (sample based estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\approx \\frac{1}{N} \\sum_i \\sum_t r(s_{i,t}, a_{i,t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/trajectories.png?raw=true\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta^* = argmax_{\\theta} E_{\\tau \\sim \\pi_{\\theta}(\\tau)}[\\sum_t r(s_t, a_t)] = argmax_{\\theta} J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $E_{\\tau \\sim \\pi_{\\theta}(\\tau)}[\\sum_t r(s_t, a_t)] = \\int \\pi_\\theta(\\tau)r(\\tau)d\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta \\pi_\\theta(\\tau)r(\\tau) d\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Convenient identity: $\\pi_\\theta \\nabla_\\theta \\log \\pi_\\theta(\\tau) = \\pi_\\theta(\\tau) \\frac{\\nabla_\\theta \\pi_\\theta(\\tau)}{\\pi_\\theta(\\tau)} = \\nabla_\\theta \\pi_\\theta(\\tau)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = \\int \\pi_\\theta \\nabla_\\theta \\log \\pi_\\theta(\\tau)r(\\tau) d\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta(\\tau)} [ \\nabla_\\theta \\log \\pi_\\theta(\\tau)r(\\tau)]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We want to find $\\theta$ that maximizes our expected return. To do this we take the gradient with respect to $\\theta$, $\\nabla_\\theta J(\\theta)$. See [Log derivative trick](http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/) for an explanation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta(\\tau)} [ \\nabla_\\theta \\log \\pi_\\theta(\\tau)r(\\tau)]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Recall: $\\pi_\\theta(\\tau) = p_{\\theta}(s_1, a_1, ..., s_T, a_T) = p(s_1) \\prod_{t=1}^T \\pi_{\\theta} (a_t | s_t) p(s_{t+1} |s_t, a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta(\\tau)} [ \\nabla_\\theta \\log ( p(s_1) \\prod_{t=1}^T \\pi_{\\theta} (a_t | s_t) p(s_{t+1} |s_t, a_t))r(\\tau)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta(\\tau)} [ \\nabla_\\theta [ \\log p(s_1) + \\sum_{t=1}^T \\log \\pi_{\\theta} (a_t | s_t) +  \\log p(s_{t+1} |s_t, a_t)r(\\tau)] ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta(\\tau)} [ (\\sum_{t=1}^T \\nabla_\\theta \\log \\pi_{\\theta} (a_t | s_t)) (\\sum_{t=1}^T r(s_t, a_t))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\\log p(s_{t+1} |s_t, a_t)$ and $p(s_1)$ do not depend on $\\theta$, thus $\\nabla_\\theta p(s_1) = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluating the Policy Gradient\n",
    "\n",
    "* Recall: $J(\\theta) = E_{\\tau \\sim \\pi_\\theta(\\tau)} [\\sum_t r(s_t, a_t)] \\approx \\frac{1}{N} \\sum_i \\sum_t r(s_{i,t}, a_{i,t})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta(\\tau)} [ (\\sum_{t=1}^T \\nabla_\\theta \\log \\pi_{\\theta} (a_t | s_t)) (\\sum_{t=1}^T r(s_t, a_t))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N [\\sum_{t=1}^T \\nabla_\\theta \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\sum_{t=1}^T r(s_{i,t}, a_{i,t})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# REINFORCE Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* sample ${\\tau^i}$ from $\\pi_\\theta(a_t | s_t)$ (run the policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N [\\sum_{t=1}^T \\nabla_\\theta \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\sum_{t=1}^T r(s_{i,t}, a_{i,t})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTePKsaxvFBPa2U9k_UVUJOm0FS2txQKQYCc879r3lAIJLh8ISq\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Sample actions from the policy to produce a trajectory, $\\tau^i$. Evaluate how good that set of actions was. i.e. sum the reward and take a gradient. Then improve the policy by performing a weight update to $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review\n",
    "* Evaluate RL objective\n",
    "    * Generate samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Evaluating policy gradient\n",
    "    * Log derivative trick\n",
    "    * Generate samples\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Understanding policy gradient\n",
    "    * Basically trial and error\n",
    "    * Make good stuff more likely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* What's wrong with policy gradient?\n",
    "    * High variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.estimator package not installed.\n",
      "tf.estimator package not installed.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "state                = env.reset()                # reset the game\n",
    "num_possible_actions = env.action_space.n         # I can go up or down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='https://miro.medium.com/proxy/1*oMSg2_mKguAGKy1C64UFlw.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = Sequential()\n",
    "agent.add(Dense(24, input_shape=(4,), activation='relu', kernel_initializer='glorot_uniform'))\n",
    "agent.add(Dense(24, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "agent.add(Dense(num_possible_actions, activation='softmax', kernel_initializer='glorot_uniform'))\n",
    "agent.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "agent.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Things to keep track of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []            # what the agent saw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rewards = []           # how good was each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gradients = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probs = agent.predict(np.vstack([state])).flatten()\n",
    "prob         = action_probs / np.sum(action_probs)\n",
    "action       = np.random.choice(num_possible_actions, 1, p=prob)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = np.zeros([num_possible_actions])\n",
    "y[action] = 1                         # the action that was actually taken\n",
    "\n",
    "gradients.append(y - prob)            # encourages the action that was taken to be taken\n",
    "states.append(state)                  # store the state\n",
    "rewards.append(reward)                # what reward did we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discounting Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99                                         # discounting factor\n",
    "running_add = 0                                      # discounted total\n",
    "discounted_rewards = np.zeros_like(rewards)\n",
    "\n",
    "for t in reversed(range(len(rewards))):              # start at end of episode \n",
    "    if rewards[t] != 0: running_add = 0\n",
    "        \n",
    "    running_add = running_add * gamma + rewards[t]   # discount reward\n",
    "    discounted_rewards[t] = running_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards          = np.vstack(rewards)\n",
    "gradients        = np.vstack(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in divide\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# there needs to be multiple samples otherwise rewards will be inf from the std\n",
    "rewards = discounted_rewards / np.std(discounted_rewards - np.mean(discounted_rewards))\n",
    "\n",
    "X = np.vstack(states)\n",
    "Y = gradients * rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 1s - loss: nan\n"
     ]
    }
   ],
   "source": [
    "history = agent.fit(\n",
    "    X, Y, \n",
    "    batch_size=32, \n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='https://github.com/jordanott/DeepLearning/blob/master/Miscellaneous/save_graph/cartpole_reinforce.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!cd ../Miscellaneous; python cart_pole.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "* A lot of material was taken from [UC Berkeley RL](http://rail.eecs.berkeley.edu/deeprlcourse/).\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
