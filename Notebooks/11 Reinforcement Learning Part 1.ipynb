{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning Part 1\n",
    "\n",
    "<center><img width=1000 src=\"https://media3.giphy.com/media/PEXjePkLmKcy4/giphy.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://am2.co/wp-content/uploads/MathCaution.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "<center><img src=\"https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/What-is-Supervised-Learning-Machine-Learning-Tutorial-Edureka.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "\n",
    "<center><img src=\"https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Unsupervised-Learning-Machine-Learning-Tutorial.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "<center><img src='https://cdn-images-1.medium.com/max/1000/1*vz3AN1mBUR2cr_jEG8s7Mg.png' width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General Problem\n",
    "\n",
    "* Maximize our reward\n",
    "* Rewards are sparse and often delayed\n",
    "* There isn't always a clear best action\n",
    "* Very little supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/lazy_opt.png?raw=true\" width=1400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Atari\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1200/1*qFHnCDhep6OmqkbVN6NY_g.gif\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Go\n",
    "<center><img src='https://storage.googleapis.com/deepmind-live-cms-alt/documents/Knowledge%2520Timeline.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://storage.googleapis.com/deepmind-live-cms-alt/documents/AZ-Blog-Fig1-Generality-Performance-Across-Games.gif\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Starcraft\n",
    "\n",
    "<center><img src='https://storage.googleapis.com/deepmind-live-cms-alt/documents/sc2-agent-vis%2520%25281%2529.gif' width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Capture the Flag\n",
    "\n",
    "<center><img src='https://storage.googleapis.com/deepmind-live-cms-alt/documents/science_results_gameplay.gif' width=1200> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why it's popular now?\n",
    "\n",
    "<center><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQYMW7qyl4GMrLvpZ0cWC5GydBmKsrdxpLbIpPMKmaIP1kvx3UP\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Advances in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Advances in reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Advances in computational capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What has RL achieved?\n",
    "\n",
    "* Acquire high degree of proficiency in domains governed by simple, known rules\n",
    "\n",
    "* Learn simple skills with raw sensory inputs, given enough experience\n",
    "* Learn from imitating enough humanprovided expert behavior\n",
    "* Basically:\n",
    "    * Video games\n",
    "    * Simulated environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are the problems?\n",
    "* Humans can learn incredibly quickly\n",
    "    * Deep RL methods are usually slow\n",
    "* Humans can reuse past knowledge\n",
    "    *Transfer learning in deep RL is an open problem\n",
    "* Not clear what the reward function should be\n",
    "* Not clear what the role of prediction should be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inferring Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"500\" src=\"https://www.youtube.com/embed/Z-eU5xZW7cU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"100%\" height=\"500\" src=\"https://www.youtube.com/embed/Z-eU5xZW7cU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where do rewards come from?\n",
    "\n",
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/basal_ganglia.png?raw=true\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Forms of Supervision\n",
    "\n",
    "* Learning from demonstrations\n",
    "     * Directly copying observed behavior\n",
    "     * Inferring rewards from observed behavior (inverse reinforcement learning)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Learning from observing the world\n",
    "    * Learning to predict\n",
    "    * Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Learning from other tasks\n",
    "    * Transfer learning\n",
    "    * Meta-learning: learning to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Decision Process\n",
    "* Mathematical formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $S$: set of possible states\n",
    "* $A$: set of possible actions\n",
    "* $R$: distribution of reward given (state, action) pair\n",
    "* $P$: transition probability i.e. distribution over next state given (state, action) pair\n",
    "* $\\gamma$: discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Atari Games\n",
    "\n",
    "<center><img src='https://www.retrogamer.net/wp-content/uploads/2014/07/Top-10-Atari-Jaguar-Games-616x410.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Goal:** Maximize the score\n",
    "* **State:** Raw pixels of the game screen\n",
    "* **Actions:** Joystick controls/buttons\n",
    "* **Reward:** Game score at each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Terminology and Notation\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1000/1*vz3AN1mBUR2cr_jEG8s7Mg.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $s_t$ - State: pixels on the screen (what Mario sees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $a_t$ - Action: for Mario right, left, up, down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\pi_{\\theta}(a_t | s_t)$ - Policy: what action to take given the state we observe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $r(s, a)$ - Reward: how good it is to be in state $s_t$ and take action $a_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $p(s_{t+1} | s_t, a_t)$ - Transition dynamics: probability of going to the next state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\tau = s_1, a_1, ..., s_T, a_T$ - Finite Trajectory: sequence of states & actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goal of Reinforcement Learning\n",
    "\n",
    "$p_\\theta(\\tau) = p_{\\theta}(s_1, a_1, ..., s_T, a_T) = p(s_1) \\prod_{t=1}^T \\pi_{\\theta} (a_t | s_t) p(s_{t+1} |s_t, a_t)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\theta^{*} = argmax_{\\theta} E_{\\tau \\sim p_{\\theta}(\\tau)}[\\sum_t r(s_t, a_t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Find the parameters that give us the highest expected reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sometimes it's very hard to know $p(s_{t+1}|s_t, a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Note: there's a difference between $p_\\theta$ and $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Immitation Learning\n",
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/imitation_learning.png?raw=true\">\n",
    "\n",
    "Bojarski et al.'16, NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/imitation_learning_nvidia.png?raw=true\" width=800>\n",
    "\n",
    "Bojarski et al.'16, NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameterized Policies\n",
    "* $\\Pi = \\{\\pi_{\\theta}, \\theta \\in R^m \\}$\n",
    "* \"A policy, $\\pi_{\\theta}$, with parameters $\\theta$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\\pi_\\theta$ is mario. The policy mario follows, $\\pi$, is determined by a set of parameters $\\theta$, **you** (the player). If you change $\\theta$ mario will take different actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Gradients\n",
    "* Problems with Q-learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Requires a descrete set of actions\n",
    "* Hard to learn exact value of every (state, action) pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Instead we want to learn a policy\n",
    "* Tells us directly what actions to take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluate a Policy\n",
    "* **Objective:** $\\theta^{*} = argmax_{\\theta} E_{\\tau \\sim p_{\\theta}(\\tau)}[\\sum_t r(s_t, a_t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $J(\\theta) = E_{\\tau \\sim p_{\\theta}(\\tau)}[\\sum_t r(s_t, a_t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We want to approximate $J(\\theta)$ (sample based estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\approx \\frac{1}{N} \\sum_i \\sum_t r(s_{i,t}, a_{i,t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/trajectories.png?raw=true\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta^* = argmax_{\\theta} E_{\\tau \\sim p_{\\theta}(\\tau)}[\\sum_t r(s_t, a_t)] = argmax_{\\theta} J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $E_{\\tau \\sim p_{\\theta}(\\tau)}[\\sum_t r(s_t, a_t)] = \\int p_\\theta(\\tau)r(\\tau)d\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta p_\\theta(\\tau)r(\\tau) d\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Convenient identity: $p_\\theta \\nabla_\\theta \\log p_\\theta(\\tau) = p_\\theta(\\tau) \\frac{\\nabla_\\theta p_\\theta(\\tau)}{p_\\theta(\\tau)} = \\nabla_\\theta p_\\theta(\\tau)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = \\int p_\\theta \\nabla_\\theta \\log p_\\theta(\\tau)r(\\tau) d\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = E_{\\tau \\sim p_\\theta(\\tau)} [ \\nabla_\\theta \\log p_\\theta(\\tau)r(\\tau)]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We want to find $\\theta$ that maximizes our expected return. To do this we take the gradient with respect to $\\theta$, $\\nabla_\\theta J(\\theta)$. See [Log derivative trick](http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/) for an explanation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = E_{\\tau \\sim p_\\theta(\\tau)} [ \\nabla_\\theta \\log p_\\theta(\\tau)r(\\tau)]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Recall: $p_\\theta(\\tau) = p_{\\theta}(s_1, a_1, ..., s_T, a_T) = p(s_1) \\prod_{t=1}^T \\pi_{\\theta} (a_t | s_t) p(s_{t+1} |s_t, a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = E_{\\tau \\sim p_\\theta(\\tau)} [ \\nabla_\\theta \\log ( p(s_1) \\prod_{t=1}^T \\pi_{\\theta} (a_t | s_t) p(s_{t+1} |s_t, a_t))r(\\tau)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = E_{\\tau \\sim p_\\theta(\\tau)} [ \\nabla_\\theta [ \\log p(s_1) + \\sum_{t=1}^T \\log \\pi_{\\theta} (a_t | s_t) +  \\log p(s_{t+1} |s_t, a_t)r(\\tau)] ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = E_{\\tau \\sim p_\\theta(\\tau)} [ (\\sum_{t=1}^T \\nabla_\\theta \\log \\pi_{\\theta} (a_t | s_t)) (\\sum_{t=1}^T r(s_t, a_t))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\\log p(s_{t+1} |s_t, a_t)$ and $p(s_1)$ do not depend on $\\theta$, thus $\\nabla_\\theta p(s_1) = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluating the Policy Gradient\n",
    "\n",
    "* Recall: $J(\\theta) = E_{\\tau \\sim p_\\theta(\\tau)} [\\sum_t r(s_t, a_t)] \\approx \\frac{1}{N} \\sum_i \\sum_t r(s_{i,t}, a_{i,t})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = E_{\\tau \\sim p_\\theta(\\tau)} [ (\\sum_{t=1}^T \\nabla_\\theta \\log \\pi_{\\theta} (a_t | s_t)) (\\sum_{t=1}^T r(s_t, a_t))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N [\\sum_{t=1}^T \\nabla_\\theta \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\sum_{t=1}^T r(s_{i,t}, a_{i,t})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# REINFORCE Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* sample ${\\tau^i}$ from $\\pi_\\theta(a_t | s_t)$ (run the policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\nabla_\\theta J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N [\\sum_{t=1}^T \\nabla_\\theta \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\sum_{t=1}^T r(s_{i,t}, a_{i,t})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTePKsaxvFBPa2U9k_UVUJOm0FS2txQKQYCc879r3lAIJLh8ISq\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Sample actions from the policy to produce a trajectory, $\\tau^i$. Evaluate how good that set of actions was. i.e. sum the reward and take a gradient. Then improve the policy by performing a weight update to $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reducing Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "* A lot of material was taken from [UC Berkeley RL](http://rail.eecs.berkeley.edu/deeprlcourse/).\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
