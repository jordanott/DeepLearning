{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning Part 2\n",
    "<center><img src='https://cdn-media-1.freecodecamp.org/images/1*rvGVriKT_aLeLKvAP16S0A.gif' width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Terminology and Notation\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1000/1*vz3AN1mBUR2cr_jEG8s7Mg.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $s_t$ - State: pixels on the screen (what Mario sees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $a_t$ - Action: for Mario right, left, up, down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $V_\\pi(s)$ - Value: how good it is to be in state $s$ when following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $Q_\\pi(s,a)$ - Value: how good it is to be in state $s$ and take action $a$ when following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\tau = s_1, a_1, ..., s_T, a_T$ - Finite Trajectory: sequence of states & actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Return\n",
    "\n",
    "* $G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum_{k=0}^{\\infty} \\gamma^kR_{t+k+1}$\n",
    "* Sum of discounted rewards going forward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\gamma \\in [0,1]$ - discount factor\n",
    "    * Penalize future rewards\n",
    "    * Rewards now are better than rewards in the future\n",
    "    * Provides mathematical convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# State Value Function\n",
    "\n",
    "* $V_\\pi(s) = \\textbf{E}_\\pi[G_t | S_t = s]$\n",
    "* Expected return from state $s$, at time $t$, following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\\begin{aligned}\n",
    "V(s) &= \\mathbb{E}[G_t \\vert S_t = s] \\\\\n",
    "&= \\mathbb{E} [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\vert S_t = s] \\\\\n",
    "&= \\mathbb{E} [R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\dots) \\vert S_t = s] \\\\\n",
    "&= \\mathbb{E} [R_{t+1} + \\gamma G_{t+1} \\vert S_t = s] \\\\\n",
    "&= \\mathbb{E} [R_{t+1} + \\gamma V(S_{t+1}) \\vert S_t = s]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Action Value Function\n",
    "\n",
    "* $Q_\\pi(s, a) = \\textbf{E}_\\pi[G_t | S_t = s, A_t=a]$\n",
    "* Expected return from state $s$, at time $t$, taking action $a$, following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://lilianweng.github.io/lil-log/assets/images/bellman_equation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recovering the State Value Function\n",
    "\n",
    "* How can I write the value function, $V_\\pi(s)$, using $Q_\\pi(s,a)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $V_\\pi(s) = \\sum_{a\\in A}Q_\\pi(s,a)\\pi(a|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\\begin{aligned}\n",
    "Q(s, a) \n",
    "&= \\mathbb{E} [R_{t+1} + \\gamma V(S_{t+1}) \\mid S_t = s, A_t = a] \\\\\n",
    "&= \\mathbb{E} [R_{t+1} + \\gamma \\mathbb{E}_{a\\sim\\pi} Q(S_{t+1}, a) \\mid S_t = s, A_t = a]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $V_\\pi(s)$ vs $Q_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $Q_\\pi(s,a)$ is typically more usefull\n",
    "    * Tells us which actions to take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $V_\\pi(s)$ is only useful if we know the transition dynamics $P(S_{t+1}|S_t, A_t)$\n",
    "    * If we know how to get from one state to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll focus on $Q_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value of Goals\n",
    "<center><img src=\"https://i.gadgets360cdn.com/large/google_deepmind_lab_1480999487929.gif\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimal Policies\n",
    "\n",
    "* The best (optimal) policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\pi_* = \\text{argmax}_\\pi Q_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameterized Action Value Function\n",
    "\n",
    "* Use neural network with parameters $\\theta$\n",
    "* $Q_\\theta(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Example: \n",
    "    * $s$ is an image (pixels of atari game)\n",
    "    * $a$ actions in game (left, right, shoot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Output Q value for all actions\n",
    "    * More efficient than passing each action through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Temporal Difference (TD) Learning\n",
    "\n",
    "* Update targets using existing extimates instead of waiting for actual results\n",
    "* This is known as **bootstrapping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* These should be equal\n",
    "* $Q_\\pi(S_t, A_t) = R_{t+1} + \\gamma \\max_{a\\in A}Q_\\pi(S_{t+1}, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Because we're approximating \n",
    "* We'll update $Q_\\theta(S_t, A_t)$ to be closer to $R_{t+1} + \\gamma \\max_{a\\in A}Q_\\theta(S_{t+1}, a)$\n",
    "* $Q_\\theta(S_t, A_t) \\approx R_{t+1} + \\gamma \\max_{a\\in A}Q_\\theta(S_{t+1}, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TD vs Monte Carlo\n",
    "\n",
    "<center><img src=\"https://lilianweng.github.io/lil-log/assets/images/TD_MC_DP_backups.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* $R_{t+1}$ is the reward we observed after our network took action $a$\n",
    "* We want to make the value of $Q_\\theta(S_{t}, a)$ be more accurate so we'll bootstrap a target using the next state we see $S_{t+1}$\n",
    "* This gives us more information and will lead to a more accurate target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning: Off Policy TD Control\n",
    "\n",
    "* Start from state $S_t$, pick $A_t = \\text{argmax}_{a\\in A}Q_\\theta(S_t, a)$\n",
    "    * $\\epsilon$ greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Get reward $R_{t+1}$ from taking action $A_t$; Go into state $S_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Update action-value function:  \n",
    "    $Q_\\theta(S_t, A_t) \\leftarrow Q_\\theta(S_t, A_t) + \\alpha(R_{t+1} + \\gamma \\max_{a\\in A}Q_\\theta(S_{t+1}, a) - Q_\\theta(S_t, A_t))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is known as the TD error $R_{t+1} + \\gamma \\max_{a\\in A}Q_\\theta(S_{t+1}, a) - Q_\\theta(S_t, A_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problems with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Targets are constantly changing\n",
    "    * Because we're bootstrapping\n",
    "    * Every time we update $Q_\\theta$ the bootstrapped targets change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Samples are sequentially correlated\n",
    "    * $S_{t+1}$ always paired with $S_t$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Store episodes $e_t=(S_t,A_t,R_t,S_{t+1})$ in replay memory $D_t={e_1,â€¦,e_t}$\n",
    "* During Q-learning updates, samples are drawn at random from the replay memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Improves data efficiency\n",
    "* Removes correlations in the observation sequences\n",
    "* Smooths over changes in the data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Periodically Updated Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Two neural networks\n",
    "    * $Q_\\theta$: Updated every time step\n",
    "    * $Q_{\\theta^{(T)}}$: Target network, updated every $C$ steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $Q_{\\theta^{(T)}}$ provides stable targets to learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Every $C$ steps $Q_{\\theta^{(T)}} \\leftarrow Q_\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning Experience Replay and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}_{(s, a, r, s_{t+1}) \\sim U(D)} \\Big[ \\big( r + \\gamma \\max_{a_{t+1}} Q_{\\theta^{(T)}}(s_{t+1}, a_{t+1}) - Q_\\theta(s, a) \\big)^2 \\Big]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://lilianweng.github.io/lil-log/assets/images/DQN_algorithm.png\" width=1000>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
