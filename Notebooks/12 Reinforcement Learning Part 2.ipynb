{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning Part 2\n",
    "<center><img src='https://cdn-media-1.freecodecamp.org/images/1*rvGVriKT_aLeLKvAP16S0A.gif' width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Administrative \n",
    "* Test 11/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Homework due 11/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Presentations 12/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Blog due 12/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"600\" src=\"https://www.youtube.com/embed/W_gxLKSsSIE\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"100%\" height=\"600\" src=\"https://www.youtube.com/embed/W_gxLKSsSIE\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='http://louiskirsch.com/assets/posts/map-reinforcement-learning/methods.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Terminology and Notation\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1000/1*vz3AN1mBUR2cr_jEG8s7Mg.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $s_t$ - State: pixels on the screen (what Mario sees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $a_t$ - Action: for Mario right, left, up, down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $V_\\pi(s)$ - Value: how good it is to be in state $s$ when following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $Q_\\pi(s,a)$ - Value: how good it is to be in state $s$ and take action $a$ when following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\tau = s_1, a_1, ..., s_T, a_T$ - Finite Trajectory: sequence of states & actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy vs Value Function\n",
    "\n",
    "<center><img src='https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/09/im_30.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Return\n",
    "\n",
    "* $G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum_{k=0}^{\\infty} \\gamma^kR_{t+k+1}$\n",
    "* Sum of discounted rewards going forward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\gamma \\in [0,1]$ - discount factor\n",
    "    * Penalize future rewards\n",
    "    * Rewards now are better than rewards in the future\n",
    "    * Provides mathematical convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# State Value Function\n",
    "\n",
    "* $V_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$\n",
    "* Expected return from state $s$, at time $t$, following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{aligned}\n",
    "V(s) &= \\mathbb{E}_\\pi[G_t \\vert S_t = s] \\\\\n",
    "&= \\mathbb{E}_\\pi [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\vert S_t = s] \\\\\n",
    "&= \\mathbb{E}_\\pi [R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\dots) \\vert S_t = s] \\\\\n",
    "&= \\mathbb{E}_\\pi [R_{t+1} + \\gamma G_{t+1} \\vert S_t = s] \\\\\n",
    "&= \\mathbb{E}_\\pi [R_{t+1} + \\gamma V(S_{t+1}) \\vert S_t = s]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bellman Expectation Equation\n",
    "\n",
    "Value information from successor states is transferred back to the current state \n",
    "\n",
    "<center><img src='https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/09/im_13.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{aligned}\n",
    "V_{\\pi}(s) &= \\mathbb{E}_\\pi[G_t | S_t = s] \\\\\n",
    "&= \\mathbb{E}_\\pi [R_{t+1} + \\gamma V(S_{t+1}) \\vert S_t = s]  \\\\\n",
    "&= \\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r + \\gamma V_{\\pi}(s')] \n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The Bellman expectation equation averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way. ([reference](https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Action Value Function\n",
    "\n",
    "* $Q_\\pi(s, a) = \\textbf{E}_\\pi[G_t | S_t = s, A_t=a]$\n",
    "* Expected return from state $s$, at time $t$, taking action $a$, following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://lilianweng.github.io/lil-log/assets/images/bellman_equation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recovering the State Value Function\n",
    "\n",
    "* How can I write the value function, $V_\\pi(s)$, using $Q_\\pi(s,a)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $V_\\pi(s) = \\sum_{a\\in A}Q_\\pi(s,a)\\pi(a|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{aligned}\n",
    "Q_\\pi(s, a) \n",
    "&= \\mathbb{E}_\\pi [R_{t+1} + \\gamma V(S_{t+1}) \\mid S_t = s, A_t = a] \\\\\n",
    "&= \\mathbb{E}_\\pi [R_{t+1} + \\gamma \\mathbb{E}_{a\\sim\\pi} [Q(S_{t+1}, a)] \\mid S_t = s, A_t = a]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $V_\\pi(s)$ vs $Q_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $Q_\\pi(s,a)$ is typically more usefull\n",
    "    * Tells us which actions to take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $V_\\pi(s)$ is only useful if we know the transition dynamics $P(S_{t+1}|S_t, A_t)$\n",
    "    * If we know how to get from one state to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll focus on $Q_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value of Goals\n",
    "<center><img src=\"https://i.gadgets360cdn.com/large/google_deepmind_lab_1480999487929.gif\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Evaluation\n",
    "\n",
    "$Q_{\\pi}(s,a) = \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r + \\gamma V_{\\pi}(s')]  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$Q_{\\pi}(s,a) = \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r + \\gamma \\sum_{a'\\in A}Q_\\pi(s',a')\\pi(a'|s')]  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Improvement\n",
    "* Act greedy with respect to $Q_{\\pi} (s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\pi' \\leftarrow argmax_{a \\in \\mathcal{A}} Q_{\\pi}(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Iteration\n",
    "\n",
    "<center><img src='https://miro.medium.com/max/3104/1*PkQDd3IdcDXI4vUVwMAqKA.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameterized Action Value Function\n",
    "\n",
    "* Use neural network with parameters $\\theta$\n",
    "* $Q_\\theta(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Example: \n",
    "    * $s$ is an image (pixels of atari game)\n",
    "    * $a$ actions in game (left, right, shoot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Output Q value for all actions\n",
    "    * More efficient than passing each action through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Q Network\n",
    "\n",
    "<center><img src='https://media.springernature.com/full/nature-static/assets/v1/image-assets/nature14236-f1.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Temporal Difference (TD) Learning\n",
    "\n",
    "* Update targets using existing extimates instead of waiting for actual results\n",
    "* This is known as **bootstrapping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* These should be equal\n",
    "* $Q_\\pi(S_t, A_t) = R_{t+1} + \\gamma \\max_{a\\in A}Q_\\pi(S_{t+1}, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Because we're approximating \n",
    "* We'll update $Q_\\theta(S_t, A_t)$ to be closer to $R_{t+1} + \\gamma \\max_{a\\in A}Q_\\theta(S_{t+1}, a)$\n",
    "* $Q_\\theta(S_t, A_t) \\approx R_{t+1} + \\gamma \\max_{a\\in A}Q_\\theta(S_{t+1}, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TD vs Monte Carlo\n",
    "\n",
    "<center><img src=\"https://lilianweng.github.io/lil-log/assets/images/TD_MC_DP_backups.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $R_{t+1}$ is the reward we observed after our network took action $a$\n",
    "* We want to make the value of $Q_\\theta(S_{t}, a)$ be more accurate so we'll bootstrap a target using the next state we see $S_{t+1}$\n",
    "* This gives us more information and will lead to a more accurate target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploration Eploitation Tradeoff\n",
    "\n",
    "<center><img src='https://miro.medium.com/max/2743/1*7axVBpiVF4VQCxxP1UNcnw.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Epsilon Greedy\n",
    "\n",
    "* Take random action with probability $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Take action specified by model with probability $1 - \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Start with high $\\epsilon$\n",
    "    * Decay it over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning: Off Policy TD Control\n",
    "\n",
    "* Start from state $S_t$, pick $A_t = \\text{argmax}_{a\\in A}Q_\\theta(S_t, a)$\n",
    "    * $\\epsilon$ greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Get reward $R_{t+1}$ from taking action $A_t$; Go into state $S_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Update action-value function:  \n",
    "\\begin{equation}\n",
    "Q_\\theta(S_t, A_t) \\leftarrow Q_\\theta(S_t, A_t) + \\alpha(R_{t+1} + \\gamma \\max_{a\\in A}Q_\\theta(S_{t+1}, a) - Q_\\theta(S_t, A_t))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is known as the TD error: \n",
    "\\begin{equation}\n",
    "R_{t+1} + \\gamma \\max_{a\\in A}Q_\\theta(S_{t+1}, a) - Q_\\theta(S_t, A_t)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problems with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Targets are constantly changing\n",
    "    * Because we're bootstrapping\n",
    "    * Every time we update $Q_\\theta$ the bootstrapped targets change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Samples are sequentially correlated\n",
    "    * $S_{t+1}$ always paired with $S_t$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Store episodes $e_t=(S_t,A_t,R_t,S_{t+1})$ in replay memory $D_t={e_1,…,e_t}$\n",
    "* During Q-learning updates, samples are drawn at random from the replay memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Improves data efficiency\n",
    "* Removes correlations in the observation sequences\n",
    "* Smooths over changes in the data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Periodically Updated Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Two neural networks\n",
    "    * $Q_\\theta$: Updated every time step\n",
    "    * $Q_{\\theta^{(T)}}$: Target network, updated every $C$ steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $Q_{\\theta^{(T)}}$ provides stable targets to learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Every $C$ steps $Q_{\\theta^{(T)}} \\leftarrow Q_\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning Experience Replay and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}_{(s, a, r, s_{t+1}) \\sim U(D)} \\Big[ \\big( r + \\gamma \\max_{a_{t+1}} Q_{\\theta^{(T)}}(s_{t+1}, a_{t+1}) - Q_\\theta(s, a) \\big)^2 \\Big]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://lilianweng.github.io/lil-log/assets/images/DQN_algorithm.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "for e in range(episodes):\n",
    "    # reset state in the beginning of each game\n",
    "    state = env.reset()\n",
    "\n",
    "    done  = False\n",
    "    while not done:\n",
    "        # agent takes an action\n",
    "        action = agent.act(state)\n",
    "        # take a step in the environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Remember the previous state, action, reward, and done\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "    \n",
    "    # learn from past experience\n",
    "    agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Take actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py - act() function\n",
    "if np.random.rand() <= self.epsilon:\n",
    "    # The agent acts randomly\n",
    "    action = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "else:\n",
    "    # Predict the reward value based on the given state\n",
    "    act_values = self.model.predict(state)\n",
    "\n",
    "    # Pick the action based on the predicted reward\n",
    "    action = np.argmax(act_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Store Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py - remember() function\n",
    "memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py - replay() function\n",
    "minibatch = random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for state, action, reward, next_state, done in minibatch:\n",
    "    target = reward\n",
    "    if not done:\n",
    "        # if there's a next state predict the q values of it\n",
    "        target = reward + self.gamma * \\\n",
    "               np.amax(self.model.predict(next_state)[0])\n",
    "        \n",
    "    # predict q values of state\n",
    "    target_f = self.model.predict(state)\n",
    "    # update the q value from the action that was taken\n",
    "    target_f[0][action] = target\n",
    "    # train\n",
    "    self.model.fit(state, target_f, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Decay Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if epsilon > epsilon_min:\n",
    "    epsilon *= epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"600\" src=\"https://www.youtube.com/embed/V1eYniJ0Rnk?start=19\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"100%\" height=\"600\" src=\"https://www.youtube.com/embed/V1eYniJ0Rnk?start=19\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN Atari Results\n",
    "\n",
    "<center><img src='https://media.nature.com/m685/nature-assets/nature/journal/v518/n7540/images/nature14236-f3.jpg'>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
