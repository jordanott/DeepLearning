{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Terminology and Notation\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*vz3AN1mBUR2cr_jEG8s7Mg.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $s_t$ - State: pixels on the screen (what Mario sees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $a_t$ - Action: for Mario right, left, up, down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $V_\\pi(s)$ - Value: how good it is to be in state $s$ when following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $Q_\\pi(s,a)$ - Value: how good it is to be in state $s$ and take action $a$ when following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\tau = s_1, a_1, ..., s_T, a_T$ - Finite Trajectory: sequence of states & actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Return\n",
    "\n",
    "* $G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum_{k=0}^{\\infty} \\gamma^kR_{t+k+1}$\n",
    "* Sum of discounted rewards going forward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\gamma \\in [0,1]$ - discount factor\n",
    "    * Penalize future rewards\n",
    "    * Rewards now are better than rewards in the future\n",
    "    * Provides mathematical convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# State Value Function\n",
    "\n",
    "* $V_\\pi(s) = \\textbf{E}_\\pi[G_t | S_t = s]$\n",
    "* Expected return from state $s$, at time $t$, following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Action Value Function\n",
    "\n",
    "* $Q_\\pi(s, a) = \\textbf{E}_\\pi[G_t | S_t = s, A_t=a]$\n",
    "* Expected return from state $s$, at time $t$, taking action $a$, following policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Recovering the State Value Function\n",
    "\n",
    "* How can I write the value function, $V_\\pi(s)$, using $Q_\\pi(s,a)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $V_\\pi(s) = \\sum_{a\\in A}Q_\\pi(s,a)\\pi(a|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $V_\\pi(s)$ vs $Q_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $Q_\\pi(s,a)$ is typically more usefull\n",
    "    * Tells us which actions to take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $V_\\pi(s)$ is only useful if we know the transition dynamics $P(S_{t+1}|S_t, A_t)$\n",
    "    * If we know how to get from one state to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll focus on $Q_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimal Policies\n",
    "\n",
    "* The best (optimal) policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\pi_* = \\text{argmax}_\\pi Q_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameterized Action Value Function\n",
    "\n",
    "* Use neural network with parameters $\\theta$\n",
    "* $Q_\\theta(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Example: \n",
    "    * $s$ is an image (pixels of atari game)\n",
    "    * $a$ actions in game (left, right, shoot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Output Q value for all actions\n",
    "    * More efficient than passing each action through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Temporal Difference Learning\n",
    "\n",
    "* Update targets using existing extimates instead of waiting for actual results\n",
    "* This is known as **bootstrapping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* These should be equal\n",
    "* $Q_\\pi(S_t, A_t) = R_{t+1} + \\gamma \\max_{a\\in A}Q_\\pi(S_{t+1}, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Because we're approximating \n",
    "* We'll update $Q_\\theta(S_t, A_t)$ to be closer to $R_{t+1} + \\gamma \\max_{a\\in A}Q_\\theta(S_{t+1}, a)$\n",
    "* $Q_\\theta(S_t, A_t) \\approx R_{t+1} + \\gamma \\max_{a\\in A}Q_\\theta(S_{t+1}, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* $R_{t+1}$ is the reward we observed after our network took action $a$\n",
    "* We want to make the value of $Q_\\theta(S_{t}, a)$ be more accurate so we'll bootstrap a target using the next state we see $S_{t+1}$\n",
    "* This gives us more information and will lead to a more accurate target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning: Off Policy TD Control\n",
    "\n",
    "* Start from state $S_t$, pick $A_t = \\text{argmax}_{a\\in A}Q_\\theta(S_t, a)$\n",
    "    * $\\epsilon$ greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Get reward $R_{t+1}$ from taking action $A_t$; Go into state $S_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Update action-value function:  \n",
    "    $Q_\\theta(S_t, A_t) \\leftarrow Q_\\theta(S_t, A_t) + \\alpha(R_{t+1} + \\gamma \\max_{a\\in A}Q_\\theta(S_{t+1}, a) - Q_\\theta(S_t, A_t))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is known as the TD error $R_{t+1} + \\gamma \\max_{a\\in A}Q_\\theta(S_{t+1}, a) - Q_\\theta(S_t, A_t)$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
