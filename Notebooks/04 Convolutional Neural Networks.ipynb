{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://scontent-sjc3-1.xx.fbcdn.net/v/t31.0-8/29663160_350326788794741_8613174572759534028_o.jpg?_nc_cat=102&_nc_oc=AQnMDomqk7LnXiW07nlDP3RLJTdr0M5jgOnTHBBo2OPkRMBUlIkqebI24ciImm7GvPm2hFq6y4MD1Q1WzusrZ1vR&_nc_ht=scontent-sjc3-1.xx&oh=d119e4b5db06bf6a5ebfc0bdaca45c18&oe=5DA5819C\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hubel and Wiesel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"https://static1.squarespace.com/static/5c28a79ca9e0286061906e43/t/5c538d7ee5e5f0059e4fcba7/1549328072148/hubel-experiment.jpg\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 1959 Receptive fields of single neurons in the cat's striate cortex\n",
    "* 1962 Receptive fields, binocular interaction and functional architecutre in cat's visual cortex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neocognitron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=70% src=\"https://www.researchgate.net/profile/Tugba_Erkoc/publication/323354835/figure/fig1/AS:601624254877707@1520449808379/The-structure-of-Neocognitron-16.png\">\n",
    "[Fukushima 1980]\n",
    "\n",
    "* Hierarchical architecture\n",
    "* Alternating between simple and complex cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://slideplayer.com/slide/775779/3/images/46/Back-propagation+of+error.jpg\" width=800>\n",
    "\n",
    "[Rumelhart 1986](https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The First Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2625/1*1TI1aGBZ4dybR6__DI9dzA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "[LeCun, Bottou, Bengio, Haffner 1998]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ImageNet Classification with Deep Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1838/1*qyc21qM0oxWEuRaj-XJKcw.png\">\n",
    "\n",
    "[Krizhevsky, Sutskever, Hinton, 2012]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/imagenet_results.png?raw=true\" width=900>\n",
    "\n",
    "(Fei-Fei Li & Justin Johnson & Serena Yeung, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/imagenet_networks.png?raw=true\" width=900>\n",
    "\n",
    "(Fei-Fei Li & Justin Johnson & Serena Yeung, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image Classification\n",
    "<center><img src=\"https://adeshpande3.github.io/assets/Cover.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image Segmentation\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*hf6J8vsX7gmHhZnbPa4y9g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Object Localization\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1200/1*dl42dsy6JIsUe9MilzL8NQ.jpeg\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Self Driving Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=100% height=\"500\" src=\"https://www.youtube.com/embed/bdQ5rsVgPuk\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=100% height=\"500\" src=\"https://www.youtube.com/embed/bdQ5rsVgPuk\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video Games\n",
    "<center><img src=\"https://skymind.ai/images/wiki/conv_agent.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*HNNByk8nAduVThhOVZtddg.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What happens if we use this for images? \n",
    "<center><img src=\"https://github.com/jordanott/CNN-Lecture/raw/de1b8c4047b3e3ae41b1a863cf306187578a9d59/Images/deep.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.estimator package not installed.\n",
      "tf.estimator package not installed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "def flatten_image():\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(x_train[0])\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.title('28x28 Image')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(x_train[0].reshape(1,-1)[:,::10])\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.title('1x784 Vector')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Images have spatial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e2c06b890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flatten_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Animation\n",
    "<center> <img src=\"https://cdn-images-1.medium.com/max/1600/1*L4T6IXRalWoseBncjRr4wQ@2x.gif\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolution\n",
    "\\begin{align} \n",
    "\\theta &= \\begin{bmatrix}\n",
    "       \\theta_{11} & \\theta_{12} & \\theta_{13} \\\\\n",
    "       \\theta_{21} & \\theta_{22} & \\theta_{23}\\\\\n",
    "       \\theta_{31} & \\theta_{32} & \\theta_{33}\n",
    "     \\end{bmatrix} \n",
    "     \\hspace{20mm}\n",
    "     X &= \\begin{bmatrix}\n",
    "       X_{11} & ... & X_{1w} \\\\\n",
    "       X_{21} & ... & X_{2w}\\\\\n",
    "       \\vdots & \\vdots & \\vdots \\\\\n",
    "       X_{h1} & ... & X_{hw}\n",
    "     \\end{bmatrix}\n",
    "\\end{align}\n",
    "* $X$: Image with height, $h$, width, $w$, and depth (color channels; RGB), $d$\n",
    "* $\\theta$: Weight matrix (kernel, filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Slide our weight matrix over $X$ and perform a multiplication at each location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Refered to as convolving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Output of Convolution\n",
    "* Write the output of the convolution in terms of $\\theta$ and $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align} \n",
    "     \\begin{bmatrix}\n",
    "       \\theta_{11}X_{11} + ... \\theta_{33}X_{33} & ... & \\theta_{11}X_{1w-3} + ... \\theta_{33}X_{3w} \\\\\n",
    "       \\vdots & \\vdots & \\vdots \\\\\n",
    "       \\theta_{11}X_{h-3,1} + ... \\theta_{33}X_{h3} & ... & \\theta_{11}X_{h-3w-3} + ... \\theta_{33}X_{hw}\n",
    "     \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolution Visualization\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*EuSjHyyDRPAQUdKCKLTgIQ.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/CNN-Lecture/raw/de1b8c4047b3e3ae41b1a863cf306187578a9d59/Images/convolution.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculating Output Dimensions\n",
    "\n",
    "\\begin{equation}\n",
    "    W_O = \\frac{(W_I - K + 2P)}{S} + 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    H_O = \\frac{(H_I - K + 2P)}{S} + 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    D_O = F\n",
    "\\end{equation}\n",
    "\n",
    "* $W_I, W_O$: width of input and output, respectively\n",
    "* $K$: dimesnion of filter\n",
    "* $F$: number of kernels\n",
    "* $S$: stride amount\n",
    "* $P$: padding amount\n",
    "* $D$: volume depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def calc_conv_output_dim(in_shape, kernel_shape, padding, stride):\n",
    "    h_out = (in_shape[0] - kernel_shape[0] + 2*padding) / stride + 1\n",
    "    w_out = (in_shape[1] - kernel_shape[1] + 2*padding) / stride + 1\n",
    "    d_out = kernel_shape[2]\n",
    "    \n",
    "    return (h_out, w_out, d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolution by Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set conv layer params\n",
    "kernel_size = 3; num_filters = 32; stride = 1; padding = 0\n",
    "# init weight kernel\n",
    "w = \n",
    "# get input\n",
    "x = x_train[0].reshape(28,28,1)\n",
    "\n",
    "# calculate output shape\n",
    "out_shape = calc_conv_output_dim(x.shape, w.shape, padding, stride)\n",
    "\n",
    "# holder for layer outputs\n",
    "output = np.zeros(shape=out_shape)\n",
    "\n",
    "for row in range(): # iterate through image rows\n",
    "    for col in range(): # iterate through image cols\n",
    "        # multiply weights with image\n",
    "        \n",
    "        # sum\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualize Output Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "for i in range(1,33):\n",
    "    plt.subplot(4,8,i); plt.title('Kernel: %d' %i)\n",
    "    plt.imshow(output[:,:,i-1]); plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z): return np.maximum(0,z)\n",
    "plt.figure(figsize=(20,8))\n",
    "for i in range(1,33):\n",
    "    plt.subplot(4,8,i); plt.title('Kernel: %d' %i)\n",
    "    plt.imshow(relu(output[:,:,i-1])); plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://external-preview.redd.it/PMSd8GVuO7hGu-JIEHyEpBRs__DHqtTDSn85Iw6fkt4.jpg?width=640&crop=smart&auto=webp&s=d398f29fe1fa16591bdab3cbd9a8f00334f90ec8\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Padding\n",
    "* What happens to the size of our output everytime we do a convolution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Add border of zeros to maintain same volume shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/padding.gif?raw=true\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/jordanott/CNN-Lecture/raw/de1b8c4047b3e3ae41b1a863cf306187578a9d59/Images/pooling.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Downsampling\n",
    "* Reduce number of parameters in network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pooling By Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_dim = 2; stride = 2\n",
    "output = np.random.randn(26,26,32)\n",
    "\n",
    "pool_output = np.zeros((output.shape[0]/pool_dim, output.shape[1]/pool_dim, output.shape[2]))\n",
    "\n",
    "for row in range():\n",
    "    for col in range():\n",
    "        pool_output = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Hint:** max over two axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualize Output Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "for i in range(1,33):\n",
    "    plt.subplot(4,8,i); plt.title('Kernel: %d' %i)\n",
    "    plt.imshow(output[:,:,i-1]); plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculating Output Dimensions\n",
    "\n",
    "\\begin{equation}\n",
    "    W_O = \\frac{(W_I - K)}{S} + 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    H_O = \\frac{(H_I - K)}{S} + 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    D_O = D_I\n",
    "\\end{equation}\n",
    "\n",
    "* $W_I, W_O$: width of input and output, respectively\n",
    "* $K$: dimesnion of filter\n",
    "* $S$: stride amount\n",
    "* $D$: volume depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "original_shape = output.shape\n",
    "flattened_shape = \n",
    "\n",
    "print 'Original:', original_shape, 'Flattened:', flattened_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fully Connected\n",
    "* Used for classification/regression output\n",
    "![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/74_blog_image_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Complete architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/jordanott/CNN-Lecture/raw/de1b8c4047b3e3ae41b1a863cf306187578a9d59/Images/cnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNN Properties\n",
    "* Weights are shared across space\n",
    "* Provides basis for translational invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class Activation Mapping\n",
    "<center><img src='http://ift.tt/2bCVSkt' width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CAM Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/jordanott/CNN-Lecture/raw/de1b8c4047b3e3ae41b1a863cf306187578a9d59/Images/code.png\">\n",
    "\n",
    "[Ott el at, 2018](http://delivery.acm.org/10.1145/3200000/3196402/p376-ott.pdf?ip=76.103.232.116&id=3196402&acc=OA&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E472D92DDD963FF6C&__acm__=1566094634_81e3dd914e0e81c147a4aa9fc9f4b077) [Ott et al, 2018](http://delivery.acm.org/10.1145/3200000/3196359/p336-ott.pdf?ip=76.103.232.116&id=3196359&acc=OA&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E472D92DDD963FF6C&__acm__=1566094683_d8dead48b16ce59bcdaec80c87800905)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Weight Receptivity Visualization\n",
    "\n",
    "<center><img src=\"https://media.arxiv-vanity.com/render-output/1314946/x3.png\" width=1000>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"550\" src=\"https://www.youtube.com/embed/AgkfIQ4IGaM\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"100%\" height=\"550\" src=\"https://www.youtube.com/embed/AgkfIQ4IGaM\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128 images in each batch\n",
    "batch_size = 128\n",
    "# 0-9 numbered images\n",
    "num_classes = 10\n",
    "# train for 20 steps\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://camo.githubusercontent.com/d440ac2eee1cb3ea33340a2c5f6f15a0878e9275/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687164656661756c742e6a7067\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# flatten the data\n",
    "x_train = x_train.reshape(-1, 28,28,1)\n",
    "x_test = x_test.reshape(-1, 28,28,1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Scale the data to be between 0 and 1\n",
    "x_train /= 255.\n",
    "x_test /= 255.\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Conv layer; 32 fileters; 3x3 weights; stride 1;      specify input shape in first layer\n",
    "model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), activation='relu', input_shape=(28,28,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# flatten the convolutional features\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(512, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# final dense layer; num_classes is number of outputs\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,    # training data to learn from \n",
    "    batch_size=batch_size, # size of batches\n",
    "    epochs=epochs, # how many iterations we train for \n",
    "    verbose=1, # type of logging\n",
    "    validation_data=(x_test, y_test)) # validation data to test on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "history.history['epochs'] = range(epochs)\n",
    "\n",
    "sns.lineplot(x='epochs', y='acc', data=history.history, label='Accuracy')\n",
    "sns.lineplot(x='epochs', y='val_acc', data=history.history, label='Val Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy')\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Keras.js\n",
    "\n",
    "* [Demo](https://transcranial.github.io/keras-js/#/mnist-cnn)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
