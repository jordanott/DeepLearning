{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://scontent-sjc3-1.xx.fbcdn.net/v/t31.0-8/29663160_350326788794741_8613174572759534028_o.jpg?_nc_cat=102&_nc_oc=AQnMDomqk7LnXiW07nlDP3RLJTdr0M5jgOnTHBBo2OPkRMBUlIkqebI24ciImm7GvPm2hFq6y4MD1Q1WzusrZ1vR&_nc_ht=scontent-sjc3-1.xx&oh=d119e4b5db06bf6a5ebfc0bdaca45c18&oe=5DA5819C\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hubel and Wiesel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"https://static1.squarespace.com/static/5c28a79ca9e0286061906e43/t/5c538d7ee5e5f0059e4fcba7/1549328072148/hubel-experiment.jpg\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 1959 Receptive fields of single neurons in the cat's striate cortex\n",
    "* 1962 Receptive fields, binocular interaction and functional architecutre in cat's visual cortex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neocognitron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=70% src=\"https://www.researchgate.net/profile/Tugba_Erkoc/publication/323354835/figure/fig1/AS:601624254877707@1520449808379/The-structure-of-Neocognitron-16.png\">\n",
    "[Fukushima 1980]\n",
    "\n",
    "* Hierarchical architecture\n",
    "* Alternating between simple and complex cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://slideplayer.com/slide/775779/3/images/46/Back-propagation+of+error.jpg\" width=800>\n",
    "\n",
    "[Rumelhart 1986](https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The First Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2625/1*1TI1aGBZ4dybR6__DI9dzA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "[LeCun, Bottou, Bengio, Haffner 1998]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ImageNet Classification with Deep Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1838/1*qyc21qM0oxWEuRaj-XJKcw.png\">\n",
    "\n",
    "[Krizhevsky, Sutskever, Hinton, 2012]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/imagenet_results.png?raw=true\" width=900>\n",
    "\n",
    "(Fei-Fei Li & Justin Johnson & Serena Yeung, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/imagenet_networks.png?raw=true\" width=900>\n",
    "\n",
    "(Fei-Fei Li & Justin Johnson & Serena Yeung, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image Classification\n",
    "<center><img src=\"https://adeshpande3.github.io/assets/Cover.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image Segmentation\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*hf6J8vsX7gmHhZnbPa4y9g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Object Localization\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1200/1*dl42dsy6JIsUe9MilzL8NQ.jpeg\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Self Driving Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HTML('<iframe width=100% height=\"600\" src=\"https://www.youtube.com/embed/bdQ5rsVgPuk\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video Games\n",
    "<center><img src=\"https://skymind.ai/images/wiki/conv_agent.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*HNNByk8nAduVThhOVZtddg.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What happens if we use this for images? \n",
    "<center><img src=\"https://github.com/jordanott/CNN-Lecture/raw/de1b8c4047b3e3ae41b1a863cf306187578a9d59/Images/deep.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "def flatten_image():\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(x_train[0])\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.title('28x28 Image')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(x_train[0].reshape(1,-1)[:,::10])\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.title('1x784 Vector')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Images have spatial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "flatten_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Animation\n",
    "<center> <img src=\"https://cdn-images-1.medium.com/max/1600/1*L4T6IXRalWoseBncjRr4wQ@2x.gif\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolution\n",
    "\\begin{align} \n",
    "\\theta &= \\begin{bmatrix}\n",
    "       \\theta_{11} & \\theta_{12} & \\theta_{13} \\\\\n",
    "       \\theta_{21} & \\theta_{22} & \\theta_{23}\\\\\n",
    "       \\theta_{31} & \\theta_{32} & \\theta_{33}\n",
    "     \\end{bmatrix} \n",
    "     \\hspace{20mm}\n",
    "     X &= \\begin{bmatrix}\n",
    "       X_{11} & ... & X_{1w} \\\\\n",
    "       X_{21} & ... & X_{2w}\\\\\n",
    "       \\vdots & \\vdots & \\vdots \\\\\n",
    "       X_{h1} & ... & X_{hw}\n",
    "     \\end{bmatrix}\n",
    "\\end{align}\n",
    "* $X$: Image with height, $h$, width, $w$, and depth (color channels; RGB), $d$\n",
    "* $\\theta$: Weight matrix (kernel, filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Slide our weight matrix over $X$ and perform a multiplication at each location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Refered to as convolving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Output of Convolution\n",
    "* Write the output of the convolution in terms of $\\theta$ and $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align} \n",
    "     \\begin{bmatrix}\n",
    "       \\theta_{11}X_{11} + ... \\theta_{33}X_{33} & ... & \\theta_{11}X_{1w-3} + ... \\theta_{33}X_{3w} \\\\\n",
    "       \\vdots & \\vdots & \\vdots \\\\\n",
    "       \\theta_{11}X_{h-3,1} + ... \\theta_{33}X_{h3} & ... & \\theta_{11}X_{h-3w-3} + ... \\theta_{33}X_{hw}\n",
    "     \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolution Visualization\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*EuSjHyyDRPAQUdKCKLTgIQ.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/CNN-Lecture/raw/de1b8c4047b3e3ae41b1a863cf306187578a9d59/Images/convolution.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculating Output Dimensions\n",
    "\n",
    "\\begin{equation}\n",
    "    W_O = \\frac{(W_I - K + 2P)}{S} + 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    H_O = \\frac{(H_I - K + 2P)}{S} + 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    D_O = F\n",
    "\\end{equation}\n",
    "\n",
    "* $W_I, W_O$: width of input and output, respectively\n",
    "* $K$: dimesnion of filter\n",
    "* $F$: number of kernels\n",
    "* $S$: stride amount\n",
    "* $P$: padding amount\n",
    "* $D$: volume depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def calc_conv_output_dim(in_shape, kernel_shape, padding, stride):\n",
    "    h_out = (in_shape[0] - kernel_shape[0] + 2*padding) / stride + 1\n",
    "    w_out = (in_shape[1] - kernel_shape[1] + 2*padding) / stride + 1\n",
    "    d_out = kernel_shape[2]\n",
    "    \n",
    "    return (h_out, w_out, d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolution by Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set conv layer params\n",
    "kernel_size = 3; num_filters = 32; stride = 1; padding = 0\n",
    "# init weight kernel\n",
    "w = \n",
    "# get input\n",
    "x = x_train[0].reshape(28,28,1)\n",
    "\n",
    "# calculate output shape\n",
    "out_shape = calc_conv_output_dim(x.shape, w.shape, padding, stride)\n",
    "\n",
    "# holder for layer outputs\n",
    "output = np.zeros(shape=out_shape)\n",
    "\n",
    "for row in range(): # iterate through image rows\n",
    "    for col in range(): # iterate through image cols\n",
    "        # multiply weights with image\n",
    "        \n",
    "        # sum\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualize Output Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "for i in range(1,33):\n",
    "    plt.subplot(4,8,i); plt.title('Kernel: %d' %i)\n",
    "    plt.imshow(output[:,:,i-1]); plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z): return \n",
    "plt.figure(figsize=(20,8))\n",
    "for i in range(1,33):\n",
    "    plt.subplot(4,8,i); plt.title('Kernel: %d' %i)\n",
    "    plt.imshow(relu(output[:,:,i-1])); plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://external-preview.redd.it/PMSd8GVuO7hGu-JIEHyEpBRs__DHqtTDSn85Iw6fkt4.jpg?width=640&crop=smart&auto=webp&s=d398f29fe1fa16591bdab3cbd9a8f00334f90ec8\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Padding\n",
    "* What happens to the size of our output everytime we do a convolution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Add border of zeros to maintain same volume shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/padding.gif?raw=true\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/jordanott/CNN-Lecture/raw/de1b8c4047b3e3ae41b1a863cf306187578a9d59/Images/pooling.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Downsampling\n",
    "* Reduce number of parameters in network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pooling By Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_dim = 2; stride = 2\n",
    "output = np.random.randn(26,26,32)\n",
    "\n",
    "pool_output = np.zeros((output.shape[0]/pool_dim, output.shape[1]/pool_dim, output.shape[2]))\n",
    "\n",
    "for row in range():\n",
    "    for col in range():\n",
    "        pool_output = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Hint:** max over two axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualize Output Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "for i in range(1,33):\n",
    "    plt.subplot(4,8,i); plt.title('Kernel: %d' %i)\n",
    "    plt.imshow(output[:,:,i-1]); plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculating Output Dimensions\n",
    "\n",
    "\\begin{equation}\n",
    "    W_O = \\frac{(W_I - K)}{S} + 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    H_O = \\frac{(H_I - K)}{S} + 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    D_O = D_I\n",
    "\\end{equation}\n",
    "\n",
    "* $W_I, W_O$: width of input and output, respectively\n",
    "* $K$: dimesnion of filter\n",
    "* $S$: stride amount\n",
    "* $D$: volume depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "original_shape = output.shape\n",
    "flattened_shape = \n",
    "\n",
    "print 'Original:', original_shape, 'Flattened:', flattened_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fully Connected\n",
    "* Used for classification/regression output\n",
    "![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/74_blog_image_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Complete architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/jordanott/CNN-Lecture/raw/de1b8c4047b3e3ae41b1a863cf306187578a9d59/Images/cnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNN Properties\n",
    "* Weights are shared across space\n",
    "* Provides basis for translational invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class Activation Mapping\n",
    "![](http://ift.tt/2bCVSkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CAM Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/jordanott/CNN-Lecture/raw/de1b8c4047b3e3ae41b1a863cf306187578a9d59/Images/code.png\">\n",
    "\n",
    "[Ott el at, 2018](http://delivery.acm.org/10.1145/3200000/3196402/p376-ott.pdf?ip=76.103.232.116&id=3196402&acc=OA&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E472D92DDD963FF6C&__acm__=1566094634_81e3dd914e0e81c147a4aa9fc9f4b077) [Ott et al, 2018](http://delivery.acm.org/10.1145/3200000/3196359/p336-ott.pdf?ip=76.103.232.116&id=3196359&acc=OA&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E472D92DDD963FF6C&__acm__=1566094683_d8dead48b16ce59bcdaec80c87800905)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128 images in each batch\n",
    "batch_size = 128\n",
    "# 0-9 numbered images\n",
    "num_classes = 10\n",
    "# train for 20 steps\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/d440ac2eee1cb3ea33340a2c5f6f15a0878e9275/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687164656661756c742e6a7067\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# flatten the data\n",
    "x_train = x_train.reshape(-1, 28,28,1)\n",
    "x_test = x_test.reshape(-1, 28,28,1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Scale the data to be between 0 and 1\n",
    "x_train /= 255.\n",
    "x_test /= 255.\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Conv layer; 32 fileters; 3x3 weights; stride 1;      specify input shape in first layer\n",
    "model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), activation='relu', input_shape=(28,28,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# flatten the convolutional features\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(512, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# final dense layer; num_classes is number of outputs\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,    # training data to learn from \n",
    "    batch_size=batch_size, # size of batches\n",
    "    epochs=epochs, # how many iterations we train for \n",
    "    verbose=1, # type of logging\n",
    "    validation_data=(x_test, y_test)) # validation data to test on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "history.history['epochs'] = range(epochs)\n",
    "\n",
    "sns.lineplot(x='epochs', y='acc', data=history.history, label='Accuracy')\n",
    "sns.lineplot(x='epochs', y='val_acc', data=history.history, label='Val Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy')\n",
    "plt.legend(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
