{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Network Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://www.houseofbots.com/images/news/4015/cover.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://images.ctfassets.net/sc14p6l3fnnh/3TxuyeJrWZcIShYarhqV3i/0e1fb927536970856fe3c8bb4c65ba7a/pytorch-1-.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/tf_vs_pytorch.png?raw=true\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensorflow (Static Computation Graph)\n",
    "\n",
    "1. Build computational graph describing our computation (including finding paths for backprop)\n",
    "2. Reuse the same graph on every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create placeholders for data; these get filled when we execute the graph.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Create Variables for the weights and initialize them with random data.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# Note that this doesn't actually perform any operations\n",
    "h = tf.matmul(x, w1); h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# Compute gradient of the loss with respect to w1 and w2\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# To update weights we need to evaluate the graph\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "# Now we have built our computational graph;enter a TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "    # Run the graph once to initialize the Variables w1 and w2.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create np arrays holding the real data for the inputs and targets \n",
    "    x_value = np.random.randn(N, 1000); y_value = np.random.randn(N, D_out)\n",
    "    for _ in range(500):\n",
    "        # Each time it executes we want to bind x_value to x and y_value to y\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2], \n",
    "                feed_dict={x: x_value, y: y_value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensorflow Keras Wrapper?!?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's already a whole library for keras\n",
    "* Why is it included in tensorflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In summary **WTF**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch (Dynamic Computation Graph)\n",
    "\n",
    "* Tensor: Like a numpy array, but can run on GPU\n",
    "* Autograd: Package for building computational graphs out of Tensors, and automatically computing gradients\n",
    "* Module: A neural network layer; may store state or learnable weights\n",
    "* I like it because the syntax is almost identical to numpy but with GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, dtype=torch.float)\n",
    "y = torch.randn(N, D_out, dtype=torch.float)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, dtype=torch.float, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([w1,w2], lr=1e-3)\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these are exactly the same operations we used to compute the forward pass using\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "PyTorch autograd looks a lot like TensorFlow: in both frameworks we define a computational graph, and use automatic differentiation to compute gradients. The biggest difference between the two is that TensorFlowâ€™s computational graphs are static and PyTorch uses dynamic computational graphs.\n",
    "\n",
    "In TensorFlow, we define the computational graph once and then execute the same graph over and over again, possibly feeding different input data to the graph. In PyTorch, each forward pass defines a new computational graph.\n",
    "\n",
    "Static graphs are nice because you can optimize the graph up front; for example a framework might decide to fuse some graph operations for efficiency, or to come up with a strategy for distributing the graph across many GPUs or many machines. If you are reusing the same graph over and over, then this potentially costly up-front optimization can be amortized as the same graph is rerun over and over.\n",
    "\n",
    "One aspect where static and dynamic graphs differ is control flow. For some models we may wish to perform different computation for each data point; for example a recurrent network might be unrolled for different numbers of time steps for each data point; this unrolling can be implemented as a loop. With a static graph the loop construct needs to be a part of the graph; for this reason TensorFlow provides operators such as tf.scan for embedding loops into the graph. With dynamic graphs the situation is simpler: since we build graphs on-the-fly for each example, we can use normal imperative flow control to perform computation that differs for each input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* If you want something high level that works fast: **Keras**\n",
    "* Need to write custom layers and do fancy computations in your network: **PyTorch**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If you hate yourself: **Tensorflow**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img width=600 src=\"https://upload.wikimedia.org/wikipedia/en/thumb/f/f7/The_Aerospace_Corporation_logo.svg/1280px-The_Aerospace_Corporation_logo.svg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://cdn.drawception.com/images/panels/2014/11-18/eASthDRM7h-3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If a robot looses signal (i.e. no GPS)\n",
    "* Want to be able to localize itself\n",
    "* Using only images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "[Video](https://www.youtube.com/watch?v=u0MVbL_RyPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PoseNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|  Input: RGB Image  |  Output: Lat,lon coordinates  |\n",
    "| ------- | -------- |\n",
    "| <img src=\"http://www.velvetblues.com/wp-content/uploads/google-street-view.jpg\" width=500> | <img src=\"https://amp.businessinsider.com/images/5c9542680cf9131e9a761712-750-571.jpg\" width=500>  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ~3 meter accuracy in large area of LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Popular Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AlexNet\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/3072/1*qyc21qM0oxWEuRaj-XJKcw.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VGG\n",
    "\n",
    "<center><img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inception\n",
    "\n",
    "<center><img src=\"https://devblogs.nvidia.com/wp-content/uploads/2015/08/image6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inception Module\n",
    "\n",
    "<center><img src=\"https://d2mxuefqeaa7sj.cloudfront.net/s_8C760A111A4204FB24FFC30E04E069BD755C4EEFD62ACBA4B54BBA2A78E13E8C_1490879611424_inception_module.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resnet\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Seunghyoung_Ryu/publication/329954455/figure/fig1/AS:725290594623488@1549934161033/The-structure-of-ResNet-12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loading Pretrained Models From Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/dune.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computer Vision Problems\n",
    "\n",
    "* Classification\n",
    "* Localization\n",
    "* Object Detection\n",
    "* Semantic Segmentation\n",
    "* Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://miro.medium.com/max/1400/1*onhKzFMWm8KcikvubonH0g.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img width=800 src=\"https://miro.medium.com/max/1276/1*0EEeHWFg7kpFKzvUKw0WSw.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Object Detection\n",
    "\n",
    "<center><img width=800 src=\"https://miro.medium.com/max/1400/1*9wzAGR0GDsI4cBUY8qHBVQ.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "<center><img width=1200 src=\"https://miro.medium.com/max/1400/1*nXlx7s4wQhVgVId8qkkMMA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img width=1000 src=\"https://miro.medium.com/max/1276/1*ratkNlE3u5cT6AChInXmXQ.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How would you solve each of these problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applications\n",
    "\n",
    "|  Self driving cars   |  Biomedical   |\n",
    "|----|----|\n",
    "| <center><img src=\"https://thumbs.gfycat.com/SociableAmazingApe-small.gif\">   |  <img src=\"https://miro.medium.com/max/1284/1*8xxwKeAUQoFp28s_gw-67w.png\">  |\n",
    "|  Shopping  |  Satellite imaging |\n",
    "|  <img src=\"https://eenews.cdnartwhere.eu/sites/default/files/styles/inner_article/public/sites/default/files/images/2018-07-20-s20_ai_autonomous_checkout_retail_cashierless_standard_cognition_amazon_go_.jpg?itok=dMQ-i57t\"> | <center><img src=\"https://miro.medium.com/max/1838/1*eu72LO87fPueTbIwic4QAQ.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Input: Image Output: Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=1000 src=\"https://cdn-images-1.medium.com/max/1600/1*JMIdlC6SitUNT4pAPzolGQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review of CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=1200 src=\"https://miro.medium.com/max/1838/1*uAeANQIOQPqWZnnuH-VEyw.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* At each layer the input is *convolved* with a weight matrix\n",
    "    * i.e. dot product the weight matrix and input as the filter slides across the space\n",
    "* Weights are shared across space\n",
    "    * the same filter (weights) are used at all locations on the input\n",
    "* After all the convolutional layers\n",
    "    * the output volume is flattened\n",
    "* Fed to fully connected layers for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review of Convolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=1000 src=\"https://miro.medium.com/max/1400/1*ciDgQEjViWLnCbmX-EeSrA.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolution as Matrix Multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_4/Convolution_With_Im2col.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Instead of sliding the weight matrix to each position in the image with a `for` loop\n",
    "* It's more effecient to perform a matrix multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transposed Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transposed\n",
    "* Deconvolution\n",
    "* Fractionally strided convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img width=600 src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS66UrIvW4fA4jglM3wLt_Oyy3k0CswWAabstS0olI5f9uWqLThXw\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why it's called the [transposed convolution](https://medium.com/activating-robotic-minds/up-sampling-with-transposed-convolution-9ae4f2df52d0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Code a Deconvolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fully Convolutional Network (FCN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1838/0*jIBAjzSynvcV-lvO.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here the architecture consists of convolutional layers followed by transposed convolutional layers. A downside of this type of architecture is that its overly simplistic and transposed convolutions on their own can lead to a checkerboarding effect in the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# UNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=1100 src=\"https://i.stack.imgur.com/EtyQs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The UNet was one of the first models developed to build on the FCN. It was originally made for bio medical imaging, in order to segment cell types. The UNet works by applying convolutions to downsample then applying transposed convolutions for upsampling, much like the FCN. However, here feature maps are copied over (grey arrows) and concatenated with the upsampled features. This helps to enhance the quality of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pyramid Scene Parsing Network (PSPNet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hszhao.github.io/projects/pspnet/figures/pspnet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A more recent approach to image segmentation is the PSP Network. Here a similar approach is taken with initial convolutional operation (CNN in image). The addition this paper makes is to segment the image into varying resolutions via global average pooling (red, orange, blue, green). Then apply a 1x1 convolution to reduction dimensionality. Then these varying resolution feature maps are upsampled via bilinear interpolation with a transposed convolution. They are all concatenated together and applied to another convolution for the output. The PSP architecture helps to incorporate global information via the varying resolution feature maps. This is important for image segmentation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# You Only Look Once ([YOLO](https://youtu.be/MPU2HistivI?t=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/yolo-network-architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Object Detection Evaluation - Intersection over Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=1000 src=\"https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_examples.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=100% height=\"500\" src=\"https://www.youtube.com/embed/MPU2HistivI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=100% height=\"500\" src=\"https://www.youtube.com/embed/MPU2HistivI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "If you really want to know the [math](https://arxiv.org/pdf/1603.07285.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
