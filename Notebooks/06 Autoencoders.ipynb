{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autoencoders\n",
    "\n",
    "<center><img width=800 src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/f_tensorflow.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning a Generative Model\n",
    "* We are given a set of examples, e.g. images of dogs\n",
    "\n",
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/model_family.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Goal:** learning a probability distribution $p(x)$ over images $x$ such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Generation:** If we sample $x_{new} \\sim p(x)$, $x_{new}$ should look like a dog (sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Density estimation:** $p(x)$ should be high if $x$ looks like a dog, and low otherwise (anomaly detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Unsupervised representation learning:** Learn what these images have in common, e.g., ears, tail, etc. (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative vs Discriminative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=70% src=\"https://i.stack.imgur.com/Xrmqg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Both are used in supervised learning where you want to learn a rule that maps input $x$ to output $y$, given a number of training examples of the form $\\{(x_i,y_i)\\}$. A generative model (e.g., naive Bayes) explicitly models the joint probability distribution $p(x,y)$ and then uses the Bayes rule to compute $p(y|x)$. On the other hand, a discriminative model (e.g., logistic regression) directly models $p(y|x)$.\n",
    "\n",
    "Some people argue that the discriminative model is better in the sense that it directly models the quantity you care about ($y$), so you don't have to spend your modeling efforts on the input $x$ (you need to compute $p(x|y)$ as well in a generative model). However, the generative model has its own advantages such as the capability of dealing with missing data, etc. For some comparison, you can take a look at this paper: [On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes](http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf)\n",
    "\n",
    "There can be cases when one model is better than the other (e.g., discriminative models usually tend to do better if you have lots of data; generative models may be better if you have some extra unlabeled data). In fact, there exists hybird models too that try to bring in the best of both worlds. See this paper for an example: [Principled hybrids of generative and discriminative models](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.89.8245&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discriminative Model (Logistic Regression)\n",
    "\n",
    "* $p(y | x, \\theta) = \\sigma(\\theta_0 + \\sum_{i=1}^n \\theta_i x_i)$\n",
    "* Directly models $p(y|x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Example Naive Bayes for Classification\n",
    "* Classify e-mails as spam $(Y = 1)$ or not spam $(Y = 0)$\n",
    "    * Let 1 : n index the words in our vocabulary (e.g., English)\n",
    "    * $x_i = 1$ if word $i$ appears in an e-mail, and 0 otherwise\n",
    "    * E-mails are drawn according to some distribution: $p(Y, x_1, ..., x_n)$\n",
    "* Suppose that words are conditionally independent given $y$:\n",
    "    *  $p(y|x_1, ..., x_n) = p(y)\\prod_{i=1}^n p(x_i|y)$\n",
    "    * **Estimate** parameters from training data. **Predict** with Bayes rule:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "p(Y=1|x_1, ..., x_n) = \\frac{p(Y=1)\\prod_{i=1}^n p(x_i|Y=1)}{\\sum_{y=\\{0,1\\}} p(Y=y|x_i) \\prod_{i=1}^n p(x_i|Y=y)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Generative Models (Naive Bayes)\n",
    "\n",
    "* $p(Y|x_1, ..., x_n) = \\frac{p(Y, x_1, ..., x_n)}{p(x_1, ..., x_n)}$\n",
    "* Explicitly models the joint probability distribution $p(x,y)$ \n",
    "* Uses the Bayes rule to compute $p(y|x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Likelihood\n",
    "* Data: $p_{data}(x)$\n",
    "* Parameters: $\\theta$\n",
    "* Model: $p_\\theta(x)$\n",
    "* Samples: $x \\sim p_{data}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='https://github.com/jordanott/DeepLearning/blob/master/Figures/data_distributions.png?raw=true' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Variables\n",
    "* **Latent:** hidden or concealed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Variable Example\n",
    "* Your **health** is a latent variable\n",
    "* There isn’t a single measurement of “health” that can be measured, it is a rather abstract concept\n",
    "* Measure physical properties from our bodies\n",
    "    * Blood pressure\n",
    "    * Cholesterol level\n",
    "    * Weight\n",
    "    * Blood sugar\n",
    "    * Temperature\n",
    "    \n",
    "* These **measurements/observations** give us a clue of a persons health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Variables for Images\n",
    "\n",
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/latent_variable_model_images.png?raw=true\">\n",
    "\n",
    "* Only shaded variables x are observed in the data (pixel values)\n",
    "* Latent variables **z** correspond to high level features\n",
    "    * If z chosen properly, $p(x|z)$ could be much simpler than $p(x)$\n",
    "    * If we had trained this model, then we could identify features via $p(z | x)$, e.g., $p(EyeColor = Blue|x)$\n",
    "* Challenge: Very difficult to specify these conditionals by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Variable Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "model:  \n",
    "$p_\\theta(x, z) = p_\\theta(x|z)p_\\theta(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* joint $p_\\theta(x, z)$\n",
    "* conditional likelihood $p_\\theta(x|z)$\n",
    "* prior $p_\\theta(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "marginalization:  \n",
    "$p_\\theta(x) = \\int p_\\theta(x,z)dz$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Encoder network \n",
    "* z = $g_\\phi(x)$\n",
    "* Translates the original high-dimension input, $x$, into the latent low-dimensional code, $z$\n",
    "* The input size is larger than the output size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decoder network\n",
    "* $x' = f_\\theta(g_\\phi(x)) = f_\\theta(z)$\n",
    "* Recovers the data from the code\n",
    "* Likely with larger and larger output layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Architecture \n",
    "<center><img src=\"https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training\n",
    "* $(\\theta, \\phi)$ are learned together\n",
    "* $\\mathbf{x} \\approx f_\\theta(g_\\phi(\\mathbf{x}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    L_\\text{AE}(\\theta, \\phi) = \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}^{(i)} - f_\\theta(g_\\phi(\\mathbf{x}^{(i)})))^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example\n",
    "* Latent dimension is 2\n",
    "\n",
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/ae.png?raw=true\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualize Latent Space\n",
    "<center><img src=\"https://www.researchgate.net/profile/Ehsan_Hosseini_Asl2/publication/275960143/figure/fig3/AS:392026551013379@1470477821195/Visualization-of-MNIST-handwritten-digits-196-higher-representation-of-digits-computed.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpolate Latent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls width=100% height=500><source src=\"https://gertjanvandenburg.com/figures/autoencoder/latent_circle.mp4\" type=\"video/mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"<video alt=\"test\" controls width=100% height=500><source src=\"https://gertjanvandenburg.com/figures/autoencoder/latent_circle.mp4\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generate Samples\n",
    "<center><img src=\"https://blog.keras.io/img/ae/vae_digits_manifold.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://scontent-sjc3-1.xx.fbcdn.net/v/t1.0-9/66808349_748631585591344_5618925140046774272_n.jpg?_nc_cat=105&_nc_oc=AQkAVDvPGtM3JS6kLpptRRy1jd4AR_ujeLo_1rLvvb7JGHuf-nI6G5gi4rDFsvC9dI5t-W8fw-A7f5Vzp4bS996M&_nc_ht=scontent-sjc3-1.xx&oh=e898657eff72ab037e870590a117688d&oe=5DB70125\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Denoising Autoencoder\n",
    "* Risk of overfitting because AE learns identity function\n",
    "* Especially when there are more parameters than data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Partially corrupt the input by adding noise\n",
    "* $\\mathcal{M}_\\mathcal{D}$ adds noise to the original input\n",
    "* $\\tilde{\\mathbf{x}} \\sim \\mathcal{M}_\\mathcal{D}(\\tilde{\\mathbf{x}} \\vert \\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training\n",
    "\n",
    "\\begin{aligned}\n",
    "\\tilde{\\mathbf{x}}^{(i)} &\\sim \\mathcal{M}_\\mathcal{D}(\\tilde{\\mathbf{x}}^{(i)} \\vert \\mathbf{x}^{(i)})\\\\\n",
    "L_\\text{DAE}(\\theta, \\phi) &= \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{x}^{(i)} - f_\\theta(g_\\phi(\\tilde{\\mathbf{x}}^{(i)})))^2\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Denoising AE Architecture\n",
    "![](https://lilianweng.github.io/lil-log/assets/images/denoising-autoencoder-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST Results\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*hfzos8xmCGjrgpTW78PFLg@2x.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variational Autoencoder\n",
    "---\n",
    "<center><img src=\"https://jaan.io/images/encoder-decoder.png\"></center>\n",
    "\n",
    "* We want to know how well the variational posterior (i.e. encoder) $q(z|x)$ approximates the true posterior $p(z|x)$ (that is unkown) \n",
    "* To do this we'll use the [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)  \n",
    "\n",
    "\\begin{align}\n",
    "\\textbf{KL}(q_\\lambda(z | x) || p(z|x)) &= \\textbf{E}_{q_\\lambda(z | x)} [\\log \\frac{q_\\lambda(z | x)}{p(z|x)}] && \\text{Definition of KL} \\\\\n",
    "&= \\textbf{E}_{q_\\lambda(z | x)} [\\log q_\\lambda(z | x)] - \\textbf{E}_{q_\\lambda(z | x)} [\\log p(x,z)] + \\log p(x) && \\text{Expanding log terms;} p(z|x) = \\frac{p(x,z)}{p(x)}\n",
    "\\end{align}\n",
    "\n",
    "* **GOAL:** Find the variational parameters $\\lambda$ that minimize $\\textbf{KL}(q_\\lambda(z | x) || p(z|x))$\n",
    "\n",
    "\\begin{equation}\n",
    "q^*_\\lambda(z | x) = argmin_\\lambda \\textbf{KL}(q_\\lambda(z | x) || p(z|x))\n",
    "\\end{equation}\n",
    "\n",
    "* **PROBLEM:** Calculating the KL is intractable because of the $p(x)$ term; this would require an integral over all $z$: $p(x) \\int_z p(x,z) dz$\n",
    "\n",
    "* **SOLUTION:** If we can find some bound on the KL we can optimize that, indirectly optimizing the KL\n",
    "    * $p(x)$ is a non-negative constant\n",
    "\n",
    "\\begin{align}\n",
    "\\textbf{KL}(q_\\lambda(z | x) || p(z|x)) \\geq \\textbf{E}_{q_\\lambda(z | x)} [\\log q_\\lambda(z | x)] - \\textbf{E}_{q_\\lambda(z | x)} [\\log p(x,z)] && \\text{Dropping the } p(x) \\text{ term provides a lower bound} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\textbf{ELBO} &= - \\textbf{E}_{q_\\lambda(z | x)} [\\log q_\\lambda(z | x)] + \\textbf{E}_{q_\\lambda(z | x)} [\\log p(x,z)]\n",
    "\\end{align}\n",
    "\n",
    "* Minimizing the KL is equivallent to maximizing the ELBO\n",
    "\n",
    "\\begin{align}\n",
    "\\textbf{ELBO} &= \\textbf{E}_{q_\\lambda(z | x)} [\\log p(x |z)] - KL(q_\\lambda(z | x) || p(z)) && \\text{Expanding this gives the original definition of ELBO above} \n",
    "\\end{align}\n",
    "\n",
    "### Loss\n",
    "\\begin{align}\n",
    "\\mathcal{L}_i(\\theta, \\phi) &= \\textbf{E}_{q_\\theta(z | x_i)} [\\log p_\\phi(x_i |z)] - KL(q_\\theta(z | x_i) || p(z))\n",
    "\\end{align}\n",
    "* The first term $\\textbf{E}_{q_\\theta(z | x_i)} [\\log p_\\phi(x_i |z)]$ serves as a reconstruction loss\n",
    "* The second term $KL(q_\\theta(z | x_i) || p(z))$ is like regularization\n",
    "    * Encourages the encoder to be close to $p(z)$\n",
    "    * Keeps representations of similar data, $x$, in the same space in $z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VAE Architecture\n",
    "![](https://lilianweng.github.io/lil-log/assets/images/vae-gaussian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reparameterization Trick\n",
    "<center><img src=\"https://lilianweng.github.io/lil-log/assets/images/reparameterization-trick.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    \\mathcal{L}(x; \\theta, \\phi) = \\textbf{E}_{q_\\phi(z|x)}[\\log p(z,x;\\theta)-\\log q_\\phi(z|x)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    = \\textbf{E}_{q_\\phi(z|x)}[\\log p(z,x;\\theta)-\\log p(z) + \\log p(z) - \\log q_\\phi(z|x)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    = \\textbf{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x)||p(z))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Take data point $x_i$\n",
    "2. Map it to $\\hat{z}$ by sampling from $q_\\phi(z|x_i)$ (encoder)\n",
    "3. Reconstruct $\\hat{x}$ by sampling from $p(x|\\hat{z}; \\theta)$ (decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What does the training objective $\\mathcal{L}(x; \\theta, \\phi)$ do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* First term encourages $\\hat{x}\\approx x_i$ ($x_i$ likely under $p(x|\\hat{z} ; \\theta)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Second term encourages $\\hat{z}$ to be likely under the prior $p(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "* [AE in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "* [From AE to Beta VAE](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
