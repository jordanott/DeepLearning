{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax  \n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y_i} = \\frac{e^{z_i}}{\\sum_{j=1}^N e^{z_j}} \n",
    "\\end{equation}\n",
    "\n",
    "* Output sums to one\n",
    "* Represent probability distribution across discrete mutually exclusive alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Derivative\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\hat{y_i}}{\\partial z_i} = \\hat{y_i} ( 1 - \\hat{y_i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    J = - \\sum_j y_j \\log \\hat{y_j}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial z_i} = - \\sum_j \\frac{\\partial J}{\\partial \\hat{y_i}} \\frac{\\partial \\hat{y_i}}{\\partial z_i} = \\hat{y_i} - y_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y_hat, y):\n",
    "    return y_hat - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch SGD Loop:\n",
    "1. Sample a batch of data\n",
    "2. Forward prop it through the graph (network), get loss\n",
    "3. Backprop to calculate the gradients\n",
    "4. Update the parameters using the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "* Because we use minibatches gradients can be noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\theta_{t+1} = \\theta_t - \\alpha \\nabla J(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} J_i (x_i, y_i, \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_{\\theta} J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_{\\theta}  J_i (x_i, y_i, \\theta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum\n",
    "* Use direction of gradients to push us forward\n",
    "* Helps to avoid local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum usually is $\\rho = 0.9$ \n",
    "\n",
    "\\begin{equation}\n",
    "    v_{t+1} = \\rho v_t + \\nabla J(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\theta_{t+1} = \\theta_t - \\alpha v_{t+1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad\n",
    "* Element wise scaling of gradient based on past sum of squares in each dimension\n",
    "* Adaptive learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_squared = 0\n",
    "while True:\n",
    "    dtheta = compute_gradients(theta)\n",
    "    grad_squared += dtheta * dtheta\n",
    "    \n",
    "    theta -= learning_rate * dtheta / (np.sqrt(grad_squared) 1+e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_squared = 0\n",
    "while True:\n",
    "    dtheta = compute_gradients(theta)\n",
    "    grad_squared += dtheta * dtheta\n",
    "    \n",
    "    theta -= learning_rate * dtheta / (np.sqrt(grad_squared) 1+e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "* $L_1, L_2$ weight penalties\n",
    "* Dropout\n",
    "* Batch Normalization\n",
    "* Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* “you want zero-mean unit-variance activations? just make them so.”\n",
    "* Compute mean and variance of each dimension\n",
    "* Normalize\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{x}^{(l)} = \\frac{x^{(l)} - E[x^{(l)}]}{\\sqrt{Var[x^{(l)}]}}\n",
    "\\end{equation}\n",
    "\n",
    "[Ioffe and Szegedy, 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sort of a regularization technique\n",
    "* Better gradient flow through network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "* Take a pretrained network (trained to classify cats)\n",
    "* Use it for a new task (classify dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These are similar taskes (cats and dogs share similar features)\n",
    "* Need much less data to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| _ | Similar dataset | Different dataset |  \n",
    "| ----- |:-----:| -----:|  \n",
    "| Small data | Train new top layer | Bummer |  \n",
    "| Big Data | Finetune a couple layers | Finetune most layers |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips\n",
    "* Watch the loss\n",
    "* Check for over fitting\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
