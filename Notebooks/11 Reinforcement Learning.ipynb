{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Atari\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*qFHnCDhep6OmqkbVN6NY_g.gif\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Go\n",
    "![](https://storage.googleapis.com/deepmind-live-cms-alt/documents/Knowledge%2520Timeline.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Starcraft\n",
    "\n",
    "![](https://storage.googleapis.com/deepmind-live-cms-alt/documents/sc2-agent-vis%2520%25281%2529.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Capture the Flag\n",
    "\n",
    "[![](https://storage.googleapis.com/deepmind-live-cms-alt/documents/science_results_gameplay.gif)](https://youtu.be/OjVxXyp7Bxw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1000/1*vz3AN1mBUR2cr_jEG8s7Mg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[![](https://www.humanjourney.us/wp-content/uploads/2017/05/ExperimentsInAltruism.jpg)](https://www.youtube.com/watch?v=Z-eU5xZW7cU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What has RL achieved?\n",
    "\n",
    "* Acquire high degree of proficiency in domains governed by simple, known rules\n",
    "\n",
    "* Learn simple skills with raw sensory inputs, given enough experience\n",
    "* Learn from imitating enough humanprovided expert behavior\n",
    "* Basically:\n",
    "    * Video games\n",
    "    * Simulated environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are the problems?\n",
    "* Humans can learn incredibly quickly\n",
    "    * Deep RL methods are usually slow\n",
    "* Humans can reuse past knowledge\n",
    "    *Transfer learning in deep RL is an open problem\n",
    "* Not clear what the reward function should be\n",
    "* Not clear what the role of prediction should be\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Decision Process\n",
    "* Mathematical formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $S$: set of possible states\n",
    "* $A$: set of possible actions\n",
    "* $R$: distribution of reward given (state, action) pair\n",
    "* $P$: transition probability i.e. distribution over next state given (state, action) pair\n",
    "* $\\gamma$: discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Policy Gradients\n",
    "* Problems with Q-learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Requires a descrete set of actions\n",
    "* Hard to learn exact value of every (state, action) pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Instead we want to learn a policy\n",
    "* Tells us directly what actions to take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameterized Policies\n",
    "* $\\Pi = \\{\\pi_{\\theta}, \\theta \\in R^m \\}$\n",
    "* \"A policy, $\\pi_{\\theta}$, with parameters $\\theta$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Define the value of a policy\n",
    "\n",
    "\\begin{equation}\n",
    "    V(\\theta) = E[ \\sum_{t \\geq 0} \\gamma^t r_t | \\pi_{\\theta} ]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **GOAL:** find optimal policy $\\theta^* = \\text{arg max}_{\\theta} V(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Gradient ascent on the policy parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# REINFORCE Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
