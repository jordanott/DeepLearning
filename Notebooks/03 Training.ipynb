{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from IPython.display import HTML\n",
    "from helper import visualization\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "figsize(20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://img.devrant.com/devrant/rant/r_1377416_RCMq1.jpg\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "* Gradient Descent\n",
    "* Loss Functions\n",
    "* Optimizers\n",
    "* Regularization\n",
    "* Augmentation\n",
    "* Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Universal Approximation Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A feedforward network with a single hidden layer containing a finite number of neurons can approximate any *continuous* function on a bounded interval to arbitrary percision [Cybenko 1989]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Given some function $f(x)$, we'd like to compute within some desired accuracy $\\epsilon > 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The guarantee is that by using enough hidden neurons we can always find a neural network whose output $g(x)$ satisfies $|g(x)−f(x)|<\\epsilon$, for all inputs $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* I'd recommend reading [this](http://neuralnetworksanddeeplearning.com/chap4.html#universality_with_one_input_and_one_output) might be on the *midterm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approximating with Step Functions\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*YryNBGz5VYBOQ-2oZqesUA@2x.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Visually what the universal approximation theorem tells us is that as as we increase the number of neurons the prediction (orange line) gets closer to the true values (blue line).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<iframe width=\"100%\" height=\"500\" src=\"https://www.youtube.com/embed/5u0jaA3qAGk\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Option 1: Random Search\n",
    "\n",
    "\n",
    "* Randomly chose weights and check if they improve the cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Terribly ineffecient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Backpropagation is more efficient than random search by a factor of the number of connections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Option 2: Numerical Gradient\n",
    "\n",
    "* **Intuition:** gradient describes rate of change of a function with respect to a variable surrounding an infinitesimally small region \n",
    "* **Finite differences**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    \\frac{f(x+h) - f(x)}{h}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* How do we compute the gradient independent of each input?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Option 3: Analytical Gradient\n",
    "\n",
    "* Chain rule\n",
    "\n",
    "\\begin{equation}\n",
    "    z = f(y); y = g(x)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We know the structure of the computational graph beforehand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why do we need to know this?\n",
    "\n",
    "* Deep learning frameworks can automatically perform backprop\n",
    "* Problems might surface related to underlying gradients when debugging your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* [Yes you should understand gradient descent](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem Statement\n",
    "* We have a loss that is a function of the input, output, and our parameters\n",
    "* We want to minimize this loss for all training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    loss = f(x, y; \\theta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss Functions\n",
    "**Regression:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Mean Squared Error (MSE)\n",
    "    * Penalizes outliers\n",
    "* Mean Absolute Error (MAE)\n",
    "    * Good when values are small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Classification:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Binary Cross-Entropy \n",
    "    * Binary classification\n",
    "* Categorical Cross-Entropy\n",
    "    * Multiple categories (e.g. image classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mean Squared Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    J = \\frac{1}{2N} (y - \\hat{y})^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 1000); y =\n",
    "plt.plot(x,y); plt.title('MSE'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mean Absolute Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    J = \\frac{1}{2N} |y - \\hat{y}|\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 1000); y = \n",
    "plt.plot(x,y); plt.title('MAE'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MSE vs MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img width=\"100%\" src=\"https://miro.medium.com/max/875/1*JTC4ReFwSeAt3kvTLq1YoA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* MAE has the same gradient everywhere\n",
    "* MAE is more robust to outliars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression (Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Should be familiar from logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 1000); y = \n",
    "\n",
    "plt.plot(x,y); plt.title('Sigmoid'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Binary Crossentropy (Negative Log Likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "P(y|x, \\theta) = \\sigma(x)^y (1 - \\sigma(x))^{1-y} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    J = - y \\log(\\hat{y}) - (1-y) \\log(1 - \\hat{y})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "plt.plot(x,-np.log(x), label='$-y\\log(\\hat{y})$'); plt.plot(x,-np.log(1-x), label='$-(1-y)\\log(1-\\hat{y})$')\n",
    "plt.title('Binary Crossentropy'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Softmax  \n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y_i} = \\frac{e^{z_i}}{\\sum_{j=1}^N e^{z_j}} \n",
    "\\end{equation}\n",
    "\n",
    "* Output sums to one\n",
    "* Represent probability distribution across discrete mutually exclusive alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x): return \n",
    "x = np.random.randn(5); y = softmax(x)\n",
    "plt.bar(np.arange(5), y); plt.xlabel('Classes'); plt.ylabel('Probability'); plt.title('Softmax'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Softmax Derivative\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\hat{y_i}}{\\partial z_i} = \\hat{y_i} ( 1 - \\hat{y_i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross-entropy Cost Function\n",
    "* Generic case for many classes\n",
    "* Convince yourself this is the same thing as binary crossentropy (i.e. two classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    J = - \\sum_j y_j \\log \\hat{y_j}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    \\frac{\\partial J}{\\partial z_i} = - \\sum_j \\frac{\\partial J}{\\partial \\hat{y_i}} \\frac{\\partial \\hat{y_i}}{\\partial z_i} = \\hat{y_i} - y_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def d_softmax(y_hat, y):\n",
    "    return y_hat - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It's often useful to rewrite mathematical expressions like this into computation graphs. This is how libraries like tensorflow handle these computations. Everytime an expression (e.g $(x+y)$) is added to the equation, a node is added to the computation graph. This way the library can easily identify the order of operations and the pathway to pass derivatives back during gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chain Rule Computation Graphs\n",
    "\n",
    "<center><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQzp_2ofryV5OI_UYxcAB55Dkmw4ulnGD_LJM2AidvUtn7cR7Rm\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"data:image/jpeg;base64,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\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backprop Example \n",
    "\n",
    "<center><img src=\"https://mohitd.github.io/images/backpropagation/forward-pass.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "<center><img src=\"https://mohitd.github.io/images/backpropagation/backward-pass.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "\n",
    "* Detailed example can be found [here](https://mohitd.github.io/2017/11/22/backpropagation.html)\n",
    "* Slides from stanford discussion on [Backpropagation and Gradients](http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds02.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mini-batch SGD Loop:\n",
    "1. Sample a batch of data\n",
    "2. Forward prop it through the graph (network), get loss\n",
    "3. Backprop to calculate the gradients\n",
    "4. Update the parameters using the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SGD\n",
    "* Because we use minibatches gradients can be noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    \\theta_{t+1} = \\theta_t - \\alpha \\nabla J(\\theta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} J_i (x_i, y_i, \\theta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    \\nabla_{\\theta} J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_{\\theta}  J_i (x_i, y_i, \\theta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Mini batch SGD is unbiased estimator but has high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://miro.medium.com/max/1400/1*PV-fcUsNlD9EgTIc61h-Ig.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Batch gradient descent** involves computing gradients over the entire training set then performing a weight update once, at the end. This is computationally expensive and inefficient because it takes so long for one update to occur. \n",
    "\n",
    "**Stochastic Gradient descent** computes the gradient and performs a weight update for each training example. \n",
    "\n",
    "**Mini-batch gradient descent** computes gradients and weight updates for small batches of the data. \n",
    "\n",
    "**Naming convention:** normally when people refer to SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning Rate\n",
    "\n",
    "<center><img src=\"https://github.com/jordanott/DeepLearning/blob/master/Figures/learning_rates.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizers\n",
    "* We've seen Stochastic Gradient Descent (SGD)\n",
    "* We can add tricks to improve learning\n",
    "    * Stability\n",
    "    * Efficiency\n",
    "    * Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Controls weight updates and gradient monitoring during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Momentum\n",
    "* Use direction of gradients to push us forward\n",
    "* Helps to avoid local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Momentum usually is $\\rho = 0.9$ \n",
    "\n",
    "\\begin{equation}\n",
    "    v_{t+1} = \\rho v_t + \\nabla J(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\theta_{t+1} = \\theta_t - \\alpha v_{t+1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "v = 0\n",
    "def momentum(theta):\n",
    "    while True:\n",
    "        dtheta = compute_gradients(theta)\n",
    "        v = rho * v + dtheta\n",
    "        theta -= learning_rate * v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AdaGrad\n",
    "* Element wise scaling of gradient based on past sum of squares in each dimension\n",
    "* Adaptive learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def AdaGrad(theta, grad_squared=0, learning_rate=0.001):\n",
    "    dtheta = compute_gradients(theta)\n",
    "    grad_squared += dtheta * dtheta\n",
    "    \n",
    "    theta -= learning_rate * dtheta / (np.sqrt(grad_squared) + 1e-7)\n",
    "    return grad_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* What happens to step size over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Decays to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* What happens with AdaGrad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Progress along “steep” directions is damped, progress along “flat” directions is accelerated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grad_squared = 0\n",
    "def RMSProp(theta):\n",
    "    while True:\n",
    "        dtheta = compute_gradients(theta)\n",
    "        grad_squared += decay_rate * grad_squared + (1 - decay_rate) * dtheta * dtheta\n",
    "\n",
    "        theta -= learning_rate * dtheta / (np.sqrt(grad_squared) + 1+e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Second moment controls variance of gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_moment = 0; second_moment = 0 \n",
    "def Adam(theta, num_iterations=1):\n",
    "    for t in range(1, num_iterations):\n",
    "        dtheta = compute_gradients(theta)\n",
    "        # momentum\n",
    "        first_moment = beta1 * first_moment + (1 - beta1) * dtheta\n",
    "        # AdaGrad / RMSProp\n",
    "        second_moment = beta2 * second_moment + (1 - beta2) * dtheta * dtheta\n",
    "        # bias correction\n",
    "        first_unbias = first_moment / (1 - beta1 ** t)\n",
    "        second_unbias = second_moment / (1 - beta2 ** t)\n",
    "        # AdaGrad / RMSProp\n",
    "        theta -= learning_rate * first_unbias / (np.sqrt(second_unbias) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sort of like RMSProp with momentum\n",
    "* First moment ~ momentum\n",
    "* Second moment ~ lowers variance of gradient estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Bias correction for the fact that first and second moment estimates start at zero\n",
    "* Very good for fast training\n",
    "* Can hurt generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Adam with beta1 = 0.9, beta2 = 0.999, and learning_rate = 1e-3 or 5e-4 is a great starting point for many models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizers\n",
    "![](https://cdn-images-1.medium.com/max/1600/0*o9jCrrX4umP7cTBA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img width=80% src=\"https://image.slidesharecdn.com/dlai2017d4l1optimization-171101161756/95/optimization-dlai-d4l1-2017-upc-deep-learning-for-artificial-intelligence-30-638.jpg?cb=1509553427\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recommendations in Practice\n",
    "\n",
    "* Adam is a good default choice in many cases\n",
    "* SGD+Momentum with learning rate decay often outperforms Adam by a bit, but requires more tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization\n",
    "* $L_1, L_2$ weight penalties\n",
    "* Dropout\n",
    "* Batch Normalization\n",
    "* Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropout\n",
    "* Randomly drop (set activities to 0) neurons in layer with probability $p$\n",
    "* Reduces dependence on single neurons\n",
    "* Increases generalization\n",
    "* Ensemble of networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=https://github.com/jordanott/CNN-Lecture/raw/de1b8c4047b3e3ae41b1a863cf306187578a9d59/Images/dropout.png width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Implementation detail:**\n",
    "* Turn dropout off at test time\n",
    "* Multiply layer activities by $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropout Test Time\n",
    "* Multiply layer activies by $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* During training $(p=\\frac{1}{2})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    \\textbf{E}[y] = \\frac{1}{4}(w_1 x_1 + w_2 x_2) + \\frac{1}{4}(w_1 x_1 + 0 x_2) + \\frac{1}{4}(0 x_1 + w_2 x_2) + \\frac{1}{4}(0 x_1 + 0 x_2)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* During testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    \\textbf{E}[y] = w_1 x_1 + w_2 x_2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "    \\textbf{E}[y] = \\frac{1}{2}(w_1 x_1 + w_2 x_2)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropout p = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=100% src=\"https://thumbs.gfycat.com/MilkyBriskAnura-size_restricted.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* “you want zero-mean unit-variance activations? just make them so.”\n",
    "* Compute mean and variance of each dimension\n",
    "* Normalize\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{x}^{(l)} = \\frac{x^{(l)} - E[x^{(l)}]}{\\sqrt{Var[x^{(l)}]}}\n",
    "\\end{equation}\n",
    "\n",
    "[Ioffe and Szegedy, 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sort of a regularization technique\n",
    "* Better gradient flow through network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Ensembles\n",
    "* Train multiple independent models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* At test time average their results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Usually gives 1-2% improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Rotation\n",
    "* Random cropping\n",
    "* Adding gaussian noise\n",
    "* Translating images (vertically and horizontally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning\n",
    "* Take a pretrained network (trained to classify cats)\n",
    "* Use it for a new task (classify dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* These are similar taskes (cats and dogs share similar features)\n",
    "* Need much less data to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning \n",
    "![](https://indico.io/wp-content/uploads/2016/02/transfer_learning_nathan.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When to use transfer learning\n",
    "\n",
    "| _ | Similar dataset | Different dataset |  \n",
    "| ----- |:-----:| -----:|  \n",
    "| Small data | Train new top layer | Bummer |  \n",
    "| Big Data | Finetune a couple layers | Finetune most layers |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Skip Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1140/1*D0F3UitQ2l5Q0Ak-tjEdJg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Allow us to build very deep networks (hundreds of layers)\n",
    "* Why are these effective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Skip connects also called residual connections. Instead of only passing the output $F(x)$ to the next layer the input, $x$, is also combined with the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/2000/1*6hF97Upuqg_LdsqWY6n_wg.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Effect of Skip Connections on the Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/dd1fad77d7c17883756b9866a31e53f52c96fe51/1-Figure1-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Li et al. 2018](https://arxiv.org/pdf/1712.09913.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tips\n",
    "\n",
    "* Watch the loss\n",
    "* Check for over fitting\n",
    "* Use dropout, batchnorm, skip connections"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
